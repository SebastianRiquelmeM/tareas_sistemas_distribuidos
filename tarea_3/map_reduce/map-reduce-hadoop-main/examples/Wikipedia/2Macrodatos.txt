2Macrodatos.txt
[[Archivo:Viegas-UserActivityonWikipedia.gif|thumb|derecha|250px|Un sistema de representación creado por [[IBM]] que muestra macrodatos que consisten en las ediciones de Wikipedia realizadas por el [[bot]] [[Pearle]]. Su visualización más racional aparece acompañada de colores y posiciones en su representación.]]
[[Archivo:DARPA Big Data.jpg|thumb|derecha|250px|"Big Data" se refiere a un fenómeno tecnológico que ha surgido desde mediados de los años ochenta. A medida que las computadoras han mejorado en capacidad y velocidad, las mayores posibilidades de almacenamiento y procesamiento también han generado nuevos problemas. Pero estos nuevos requisitos, que pueden observarse en patrones y tendencias nunca antes vistos en el manejo de estos conjuntos de datos fenomenalmente grandes, pueden ser difíciles de implementar sin nuevas herramientas analíticas que permitan ir orientando a los usuarios, destacando los posibles puntos de interés. El programa XDATA de DARPA y la comunidad de investigadores y artistas que se han reunido allí, serán esenciales para avanzar en el estado del arte relativo a los llamados 'macrodatos'.]]

Los '''macrodatos''',<ref>{{Cita noticia|título=Macrodatos e inteligencia de datos, alternativas a big data|url=http://www.fundeu.es/recomendacion/macrodatosalternativa-abig-data-1582/|fechaacceso=11 de abril de 2017}}</ref> también llamados '''datos  masivos''', '''inteligencia de datos''', '''datos a gran escala''' o '''''big data''''' (terminología en [[idioma inglés]] utilizada comúnmente) es un término que hace referencia a [[conjuntos de datos]] tan grandes y complejos que precisan de [[aplicación informática|aplicaciones informáticas]] no tradicionales de [[procesamiento de datos]] para tratarlos adecuadamente. Los datos son la reproducción simbólica de un atributo o variable cuantitativa o cualitativa; según la RAE «Información sobre algo concreto que permite su conocimiento exacto o sirve para deducir las consecuencias derivadas de un hecho».<ref>{{cita web|apellidos1=Musicco|nombre1=Daniela|título=Data drive / human drive: el reto de la Data Comunicación|url=https://comunicacionyhombre.com/article/data-drive-human-drive-reto-la-data-comunicacion/|obra=COMUNICACIÓN Y HOMBRE|fechaacceso=12 de junio de 2020}}</ref> Por ende, los procedimientos usados para encontrar patrones repetitivos dentro de esos datos son más sofisticados y requieren un software especializado. En textos científicos en español, con frecuencia se usa directamente el término en inglés ''big data'', tal como aparece en el ensayo de [[Viktor Mayer-Schönberger|Viktor Schönberger]] ''La revolución de los datos masivos''.<ref>{{Cita noticia|título="Los datos masivos (o big data) son el nuevo oro"|url=http://www.eldiario.es/turing/Big-data_0_161334397.html|fechaacceso=23 de mayo de 2017|periódico=eldiario.es|idioma=es}}</ref><ref>{{cita publicación|apellidos1=Hernández García|nombre1=Claudia|título=Big data: o cómo los datos masivos están cambiando el mundo|publicación=¿Cómo ves?|fecha=diciembre del 2018|volumen=21|número=241|páginas=8-13|url=www.comoves.unam.mx|fechaacceso=2 de diciembre de 2018|editorial=Dirección General de Divulgación de la Ciencia (UNAM)|ubicación=Ciudad de México|idioma=español}}</ref>

El uso moderno del término "big data" tiende a referirse al análisis del comportamiento del usuario, extrayendo valor de los datos almacenados, y formulando predicciones a través de los patrones observados. La disciplina dedicada a los datos masivos se enmarca en el sector de las [[tecnologías de la información y la comunicación]]. Esta disciplina se ocupa de todas las actividades relacionadas con los sistemas que manipulan grandes [[Conjunto de datos|conjuntos de datos]]. 

Las dificultades más habituales vinculadas a la gestión de estos grandes volúmenes de datos se centran en la recolección y el almacenamiento de los mismos,<ref>Kusnetzky, Dan. What is "Big Data?". ZDNet. http://blogs.zdnet.com/virtualization/?p=1708 {{Wayback|url=http://blogs.zdnet.com/virtualization/?p=1708 |date=20100221024502 }}</ref> en las búsquedas, las comparticiones, y los análisis,<ref>Vance, Ashley. Start-Up Goes After Big Data With Hadoop Helper. New York Times Blog. 22 de abril de 2010. http://bits.blogs.nytimes.com/2010/04/22/start-up-goes-after-big-data-with-hadoop-helper/?dbk</ref> y en las visualizaciones y representaciones. La tendencia a manipular enormes volúmenes de datos se debe en muchos casos a la necesidad de incluir dicha información para la creación de informes estadísticos y modelos predictivos utilizados en diversas materias, como los análisis sobre negocios, sobre publicidad, sobre enfermedades infecciosas, sobre el espionaje y el seguimiento a la población, o sobre la lucha contra el crimen organizado.<ref>Cukier, K. (25 February 2010). «Data, data everywhere».'' The Economist''. http://www.economist.com/specialreports/displaystory.cfm?story_id=15557443</ref>


El límite superior de procesamiento ha ido creciendo a lo largo de los años.<ref>{{Cita web|url=https://www.malagahoy.es/malaga/crecimiento-Big-Data_0_1285671918.html|título=El imparable crecimiento del uso del Big Data|fechaacceso=23 de octubre de 2018|autor=Málaga Hoy|sitioweb=https://www.malagahoy.es}}</ref> Se estima que el mundo almacenó unos 5 ''[[Zettabyte|zettabytes]]'' en [[2014]]. Si se pone esta información en libros, convirtiendo las imágenes y todo eso a su equivalente en letras, se podría hacer 4500 pilas de libros que lleguen hasta el sol.<ref name="theclinic.cl2">Martin Hilbert, experto en redes digitales: “Obama y Trump usaron el Big Data para lavar cerebros” http://www.theclinic.cl/2017/01/19/martin-hilbert-experto-redes-digitales-obama-trump-usaron-big-data-lavar-cerebros/</ref>


Los científicos con cierta regularidad encuentran límites en el análisis debido a la gran cantidad de datos en ciertas áreas, tales como la [[meteorología]], la [[genómica]],<ref>Community cleverness required. Nature, 455(7209), 1. 2008. http://www.nature.com/nature/journal/v455/n7209/full/455001a.html</ref> la [[conectómica]] (una aproximación al estudio del cerebro; en inglés:Connectomics; en francés: Conectomique), las complejas simulaciones de procesos físicos<ref>Sandia sees data management challenges spiral. HPC Projects. 4 August 2009. {{cita web|url=http://www.hpcprojects.com/news/news_story.php?news_id=922|título=Copia archivada|fechaacceso=22 de abril de 2011|urlarchivo=https://web.archive.org/web/20110511011635/http://www.hpcprojects.com/news/news_story.php?news_id=922|fechaarchivo=11 de mayo de 2011}}</ref> y las investigaciones relacionadas con los procesos biológicos y ambientales.<ref>Reichman,O.J., Jones, M.B., and Schildhauer, M.P. 2011. Challenges and Opportunities of Open Data in Ecology. Science 331(6018): 703-705.DOI:10.1126/science.1197962</ref>

Los ''[[Conjunto de datos|data sets]]'' crecen en volumen debido en parte a la recolección masiva de información procedente de los [[Red de sensores|sensores inalámbricos]] y los dispositivos móviles (por ejemplo las [[VANET]]), el constante crecimiento de los históricos de aplicaciones (por ejemplo de los [[Log (informática)|registros]]), las cámaras (sistemas de [[teledetección]]), los [[Micrófono|micrófonos]], los lectores de [[RFID|identificación por radiofrecuencia]].<ref>Hellerstein, Joe. Parallel Programming in the Age of Big Data. Gigaom Blog. 9 November 2008. http://gigaom.com/2008/11/09/mapreduce-leads-the-way-for-parallel-programming/</ref><ref>Segaran, Toby and Hammerbacher, Jeff. Beautiful Data. 1st Edition. O'Reilly Media. Pg 257.</ref>

La capacidad tecnológica per cápita a nivel mundial para almacenar datos se dobla aproximadamente cada cuarenta meses desde los [[años 1980]].<ref name="HilbertLopez20112">[http://www.sciencemag.org/content/332/6025/60 «The World’s Technological Capacity to Store, Communicate, and Compute Information.»] Martin Hilbert y Priscila López (2011), ''[[Science]]'', 332(6025), 60-65; free access to the article through here: martinhilbert.net/WorldInfoCapacity.html</ref> Se estima que en [[2012]] cada día fueron creados cerca de 2.5 trillones de ''bytes'' de datos.<ref>[http://www-01.ibm.com/software/data/bigdata/]</ref>

Los [[sistema de gestión de bases de datos relacionales|sistemas de gestión de bases de datos relacionales]] y los paquetes de software utilizados para visualizar datos, a menudo tienen dificultades para manejar big data. Este trabajo puede requerir "''un software masivamente paralelo que se ejecute en decenas, cientos o incluso miles de servidores''".<ref>{{Cita web|url=https://queue.acm.org/detail.cfm?id=1563874|título="The Pathologies of Big Data"|autor=Jacobs, A.|fecha=6 Julio, 2009|editorial=ACMQueue}}</ref> Lo que califica como "big data" varía según las capacidades de los usuarios y sus herramientas, y las capacidades de expansión hacen que big data sea un objetivo en movimiento. "Para algunas organizaciones, enfrentar cientos de [[Gigabyte|gigabytes]] de datos por primera vez puede provocar la necesidad de reconsiderar las opciones de administración de datos. Para otros, puede tomar decenas o cientos de [[Terabyte|terabytes]] antes de que el tamaño de los datos se convierta en una consideración importante".<ref>{{Cita libro|apellidos=Magoulas, Roger|nombre=Lorica, Ben|título=Release 2.0|url=http://assets.en.oreilly.com/1/event/54/mdw_online_bigdata_radar_pdf.pdf|año=Febrero, 2009|editorial=O'Reilly Media|ubicación=Sebastopol CA|idioma=Inglés|capítulo="Introduction to Big Data"}}</ref>

== Definición ==
El término ha estado en uso desde la [[década de 1990]], y algunos otorgan crédito a [[John Mashey]]<ref>{{Cita libro|apellidos=Mashey|nombre=John R.|título=Big Data ... and the Next Wave of InfraStress|url=http://static.usenix.org/event/usenix99/invited_talks/mashey.pdf|año=1998|editorial=Usenix|idioma=Inglés}}</ref> por popularizarlo. ''Big data'' o macrodatos es un término que hace referencia a una cantidad de datos tal que supera la capacidad del software convencional para ser capturados, administrados y procesados en un tiempo razonable. El volumen de los datos masivos crece constantemente. En el 2012, se estimaba su tamaño de entre una docena de [[terabytes]] hasta varios [[petabyte]] de datos en un único conjunto de datos. En la metodología [[MIKE2.0]], dedicada a investigar temas relacionados con la [[gestión de información]], definen ''big data''<ref>[http://mike2.openmethodology.org/wiki/Big_Data_Definition Big Data Definition]</ref> en términos de permutaciones útiles, complejidad y dificultad para borrar registros individuales.

Se ha definido también como datos lo suficientemente masivos como para poner de relieve cuestiones y preocupaciones en torno a la efectividad del anonimato desde una perspectiva más práctica que teórica.<ref>{{Obra citada|título=Big Ethics for Big Data|autor=Douglas Patterson|fecha=2012}}</ref>

En el 2001, en un informe de investigación que se fundamentaba en congresos y presentaciones relacionadas,<ref>{{cita web|url=http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf|título=3D Data Management: Controlling Data Volume, Velocity and Variety|fechaacceso=6 de febrero de 2001|apellido=Douglas|nombre=Laney|editorial=Gartner}}</ref> la META Group (ahora [[Gartner (empresa)|Gartner]]) definía el crecimiento constante de datos como una oportunidad y un reto para investigar en el volumen, la velocidad y la variedad. Gartner' continúa usando datos masivos como referencia.<ref>{{cita web|url=http://www.gartner.com/it/page.jsp?id=1731916|título=Gartner Says Solving 'Big Data' Challenge Involves More Than Just Managing Volumes of Data|fechaacceso=13 de julio de 2011|apellido=Beyer|nombre=Mark|editorial=Gartner}}</ref> Además, grandes proveedores del mercado de datos masivos están desarrollando soluciones para atender las demandas más críticas sobre cómo procesar tal cantidad de datos, como MapR y Cloudera.

Una definición de 2016 establece que "Big data representa los activos de información caracterizados por un volumen, velocidad y variedad tan altos que requieren una tecnología específica y métodos analíticos para su transformación en valor".<ref>{{Cita libro|apellidos=De Mauro, Greco, Grimaldi|nombre=Andrea, Marco, Michele|título=A Formal definition of Big Data based on its essential Features|url=https://www.emeraldinsight.com/doi/abs/10.1108/LR-06-2015-0061|año=2016|editorial=Emerald Group Publishing|idioma=Inglés}}</ref> Además, algunas organizaciones agregan una nueva V, veracidad para describirlo,<ref>{{Cita web|url=https://www.villanovau.com/resources/bi/what-is-big-data/#.W6-02y_SFpg|título=What is Big Data?|editorial=Villanova University}}</ref> revisionismo cuestionado por algunas autoridades de la industria.<ref>{{Cita web|url=https://www.informationweek.com/big-data/big-data-analytics/big-data-avoid-wanna-v-confusion/d/d-id/1111077|título=Big Data: Avoid 'Wanna V' Confusion|editor=InformationWeek}}</ref> Las tres V (volumen, variedad y velocidad) se han ampliado a otras características complementarias del big data: 

* [[Aprendizaje automático]]: los grandes datos a menudo no preguntan por qué y simplemente detectan los patrones.<ref>{{Cita libro|apellidos=Mayer-Schönberger, Cukier|nombre=Viktor, Kenneth|título=Big Data: A Revolution that Will Transform how We Live, Work, and Think|url=https://books.google.com.ar/books?id=HpHcGAkFEjkC&hl=es|año=2013|editor=Houghton Mifflin Harcourt}}</ref>
* [[Huella digital]]: el Big Data es a menudo un subproducto libre de costo de la interacción digital.

Una definición de 2018 establece que "Big Data es donde se necesitan herramientas informáticas paralelas para manejar los datos", y señala: "Esto representa un cambio distinto y claramente definido en la informática utilizada a través de teorías de programación paralelas y pérdidas de algunas de las garantías y capacidades hechas por el [[Modelo relacional|modelo relacional de Codd]]".<ref>{{Cita libro|apellidos=Fox|nombre=Charles|título=Data Science for Transport|url=https://www.springer.com/us/book/9783319729527|año=2018|editorial=Springer International Publishing|isbn=978-3-319-72952-7}}</ref>

La creciente madurez del concepto describe de manera clara y bien nítida, la diferencia entre "''Big Data (Datos a gran escala)''" y "''Business intelligence (Inteligencia empresarial)''":

* La Business intelligence usa estadísticas descriptivas con datos con alta densidad de información para medir cosas, detectar tendencias, etc.
* Por su parte, el Big Data usa estadísticas inductivas y conceptos de identificación de sistemas no lineales,<ref>{{Cita libro|apellidos=Billings|nombre=Stephen A.|título=Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains|url=https://books.google.com.ar/books/about/Nonlinear_System_Identification.html?id=SaQ2AAAAQBAJ&source=kp_cover&redir_esc=y|año=2013|editorial=John Wiley & Sons|isbn=9781118535554}}</ref> para inferir leyes (regresiones, relaciones no lineales y efectos causales) a partir de grandes conjuntos de datos con baja densidad de información, con la finalidad de revelar relaciones y dependencias, o para realizar predicciones de resultados y comportamientos.<ref>{{Cita web|url=http://archives.lesechos.fr/archives/cercle/2013/04/03/cercle_69222.htm|título=Big Data car Low-Density Data ? La faible densité en information comme facteur discriminant|autor=Pierre Delort|fecha=3 de abril de 2013|editor=Les Echos|idioma=Francés}}</ref>

== Características ==
Los macrodatos se pueden describir por las siguientes características:<ref>[https://web.archive.org/web/20180731105912/https://spotlessdata.com/blog/big-datas-fourth-v Big Data's Fourth V]</ref>

* Volumen: la cantidad de datos generados y guardados.
* Variedad: el tipo y naturaleza de los datos para ayudar a las personas a analizar los datos y usar los resultados de forma eficaz. Los macrodatos usan textos, imágenes, audio y vídeo. También completan pedazos ¿Qué pedazos? pedidos a través de la fusión de datos.
* Velocidad: en este contexto, la velocidad a la cual se generan y procesan los datos para cumplir las exigencias y desafíos de su análisis.
* Veracidad: la [[calidad de datos|calidad de los datos]] capturados puede variar mucho y así afectar a los resultados del análisis.
*Valor: los datos generados deben ser útiles, accionables y tener valor.<ref>{{Cita web|url=https://www.datahack.es/cinco-v-big-data/|título=Las cinco V’s del Big Data - datahack, especialistas en Big Data, más que una escuela y un máster|fechaacceso=16 de octubre de 2018|sitioweb=www.datahack.es|idioma=es-ES|urlarchivo=https://web.archive.org/web/20181016203319/https://www.datahack.es/cinco-v-big-data/|fechaarchivo=16 de octubre de 2018}}</ref>

== Arquitectura ==
Los repositorios de big data han existido en muchas formas, a menudo creadas por corporaciones con una necesidad especial. Históricamente, los proveedores comerciales ofrecían sistemas de administración de bases de datos paralelos para big data a partir de la década de 1990. Durante muchos años, WinterCorp publicó un informe de base de datos más grande.<ref>{{Cita web|url=http://www.eweek.com/database/survey-biggest-databases-approach-30-terabytes|título=Survey: Biggest Databases Approach 30 Terabytes|autor=Matthew Hicks|fecha=8 de noviembre de 2003}}</ref>

[[Teradata]] Corporation en 1984, comercializó el sistema de procesamiento paralelo DBC 1012. Los sistemas Teradata fueron los primeros en almacenar y analizar 1 terabyte de datos en 1992. Los discos duros eran de 2,5{{esd}}GB en 1991, por lo que la definición de big data evoluciona continuamente según la [[Ley de Kryder]]. Teradata instaló el primer sistema basado en [[Sistema de gestión de bases de datos relacionales|RDBMS]] de clase petabyte en 2007. A partir de 2017, hay unas pocas docenas de bases de datos relacionales de Teradata de clase Petabyte instaladas, la mayor de las cuales excede de 50{{esd}}PB. Los sistemas hasta 2008 eran datos relacionales estructurados al 100{{esd}}%. Desde entonces, Teradata ha agregado tipos de datos no estructurados, incluidos [[Extensible Markup Language|XML]], [[JSON]] y [[Apache_Avro|Avro]].

En 2000, Seisint Inc. (ahora [[LexisNexis Group]]) desarrolló un marco de intercambio de archivos distribuido basado en [[C++]] para el almacenamiento y consultas de datos. El sistema almacena y distribuye datos estructurados, semiestructurados y no estructurados en varios servidores. Los usuarios pueden crear consultas en un dialecto de C++ llamado [[ECL]]. ECL utiliza un método de "aplicar esquema en lectura" para inferir la estructura de los datos almacenados cuando se consulta, en lugar de cuando se almacena. En 2004, LexisNexis adquirió Seisint Inc.<ref>{{Cita noticia|apellidos=O'Harrow Jr.|nombre=Robert|título=LexisNexis To Buy Seisint For $775 Million|url=http://www.washingtonpost.com/wp-dyn/articles/A50577-2004Jul14.html?noredirect=on|fecha=15 de julio de 2004|periódico=Washington Post}}</ref> y en 2008 adquirió ChoicePoint, Inc.<ref>{{Cita noticia|apellidos=Nakashima, O'Harrow Jr.|nombre=Ellen, Robert|título=LexisNexis Parent Set to Buy ChoicePoint|url=http://www.washingtonpost.com/wp-dyn/content/article/2008/02/21/AR2008022100809.html|fecha=22 de febrero de 2008|periódico=Washington Post}}</ref>y su plataforma de procesamiento paralelo de alta velocidad. Las dos plataformas se fusionaron en sistemas HPCC (o cluster de computación de alto rendimiento) y en 2011, HPCC fue de código abierto bajo la licencia [[Apache 2.0 License|Apache v2.0]]. [[Quantcast File System]] estuvo disponible aproximadamente al mismo tiempo.<ref>{{Cita web|url=https://www.datanami.com/2012/10/01/quantcast_opens_exabyte_ready_file_system/|título=Quantcast Opens Exabyte-Ready File System|autor=Nicole Hemsoth}}</ref>

El [[Organización Europea para la Investigación Nuclear|CERN]] y otros experimentos de física han recopilado grandes conjuntos de datos durante muchas décadas, generalmente analizados a través de computadoras de alto rendimiento ([[Supercomputadora|supercomputadores]]) en lugar de las arquitecturas de mapas reducidos de productos, que generalmente se refieren al movimiento actual de "big data".

En 2004, [[Google]] publicó un documento sobre un proceso llamado [[MapReduce]]<nowiki/> que utiliza una arquitectura similar. El concepto MapReduce proporciona un modelo de procesamiento en paralelo, y se lanzó una implementación asociada para procesar grandes cantidades de datos. Con MapReduce, las consultas se dividen y distribuyen a través de nodos paralelos y se procesan en paralelo (el paso del Mapa). Los resultados se recopilan y se entregan (el paso Reducir). El marco fue muy exitoso, por lo que otros quisieron replicar el algoritmo. Por lo tanto, una implementación del marco MapReduce fue adoptada por un proyecto de código abierto Apache llamado [[Hadoop]].<ref>{{Cita publicación|url=http://static.googleusercontent.com/media/research.google.com/es//archive/mapreduce-osdi04.pdf|título=MapReduce: Simplified Data Processing on Large Clusters|apellidos=Dean, Ghemawat|nombre=Jeffrey, Sanjay|fecha=2004|publicación=Search Storage}}</ref>Apache Spark se desarrolló en 2012 en respuesta a las limitaciones del paradigma MapReduce, ya que agrega la capacidad de configurar muchas operaciones (no solo el mapa seguido de la reducción).

MIKE2.0 es un enfoque abierto para la administración de la información que reconoce la necesidad de revisiones debido a las implicaciones de big data identificadas en un artículo titulado "Oferta de soluciones de Big Data".<ref>{{Cita web|url=http://mike2.openmethodology.org/wiki/Big_Data_Solution_Offering|título=Big Data Solution Offering|editorial=MIKE 2.0}}</ref>La metodología aborda el manejo de big data en términos de permutaciones útiles de fuentes de datos, complejidad en interrelaciones y dificultad para eliminar (o modificar) registros individuales.<ref>{{Cita web|url=http://mike2.openmethodology.org/wiki/Big_Data_Definition|título=Big Data Definition|editorial=MIKE 2.0}}</ref>

Los estudios de 2012 mostraron que una arquitectura de capas múltiples es una opción para abordar los problemas que presenta el big data. Una arquitectura paralela distribuida distribuye datos entre múltiples servidores; estos entornos de ejecución paralela pueden mejorar drásticamente las velocidades de procesamiento de datos. Este tipo de arquitectura inserta datos en un [[Sistema de gestión de bases de datos|DBMS]] paralelo, que implementa el uso de los marcos MapReduce y Hadoop. Este tipo de marco busca hacer que el poder de procesamiento sea transparente para el usuario final mediante el uso de un servidor de aplicaciones para el usuario.<ref>{{Cita publicación|url=http://revistaie.ase.ro/content/62/12%20-%20Boja.pdf|título=Distributed Parallel Architecture for "Big Data"|apellidos=Boja, Pocovnicu, Bătăgan|nombre=Catalin, Adrian, Lorena|fecha=2012|publicación=Informatica Economică|número=vol. 16, no. 2}}</ref>

El análisis de big data para aplicaciones de fabricación se comercializa como una arquitectura 5C (conexión, conversión, cibernética, cognición y configuración).<ref>{{Cita web|url=http://www.imscenter.net/cyber-physical-platform|título=5C Architecture, Introduced by IMS Center for Cyber-Physical Systems in Manufacturing|editorial=Imscenter.net|fechaacceso=29 de septiembre de 2018|urlarchivo=https://web.archive.org/web/20160527175337/http://www.imscenter.net/cyber-physical-platform|fechaarchivo=27 de mayo de 2016}}</ref>

El [[lago de datos]] permite que una organización cambie su enfoque del control centralizado a un modelo compartido para responder a la dinámica cambiante de la administración de la información. Esto permite una segregación rápida de datos en el lago de datos, lo que reduce el tiempo de sobrecarga.<ref>{{Cita libro|apellidos=Wills|nombre=John|título=Solving key business challenges with a Big Data Lake|url=https://www.hcltech.com/sites/default/files/solving_key_businesschallenges_with_big_data_lake_0.pdf|año=2014|editorial=HCL}}</ref><ref>{{Cita publicación|url=https://secplab.ppgia.pucpr.br/files/papers/2015-0.pdf|título=Method for Testing the Fault Tolerance of MapReduce Frameworks|apellidos=Marynowski, Santin, Pimentel|nombre=Joa ̃o Eugenio, Altair Olivo, Andrey Ricardo|fecha=14 de febrero de 2015|publicación=Computer Networks}}</ref>

== Tecnología ==
Existen muchísimas herramientas para el manejo de big data. Algunos ejemplos incluyen [[Apache_Hadoop]], [[NoSQL]], [[Apache_Cassandra]], [[inteligencia empresarial]], [[aprendizaje automático]] y [[MapReduce]]. Estas herramientas tratan con algunos de los tres tipos de ''big data'':<ref>{{cita publicación|apellidos1=Purcell|nombre1=Bernice|título=The emergence of Big Data technology and Analytics|publicación=Holy Family University|fecha=2013|fechaacceso=18 de octubre de 2014}}</ref>

* Datos estructurados: datos que tienen bien definidos su longitud y su formato, como las fechas, los números o las cadenas de caracteres. Se almacenan en tablas. Un ejemplo son las [[bases de datos]] relacionales y los [[Almacén de datos|almacenes de datos]].
* Datos no estructurados: datos en el formato tal y como fueron recolectados, carecen de un formato específico. No se pueden almacenar dentro de una tabla ya que no se puede desgranar su información a tipos básicos de datos. Algunos ejemplos son los [[PDF]], documentos multimedia, [[correos electrónicos]] o documentos de texto.
* Datos semiestructurados: datos que no se limitan a campos determinados, pero que contiene marcadores para separar los diferentes elementos. Es una información poco regular como para ser gestionada de una forma estándar. Estos datos poseen sus propios [[metadatos]] semiestructurados<ref>{{cita publicación|apellidos1=Lopez García|nombre1=David|título=Analysis of the possibilities of use of Big Data in organizations|fecha=2012-2013|url=http://bucserver01.unican.es/xmlui/bitstream/handle/10902/4528/TFM%20-%20David%20L%C3%B3pez%20Garc%C3%ADaS.pdf?sequence=1|fechaacceso=18 de octubre de 2014|urlarchivo=https://web.archive.org/web/20150101213425/http://bucserver01.unican.es/xmlui/bitstream/handle/10902/4528/TFM%20-%20David%20L%C3%B3pez%20Garc%C3%ADaS.pdf?sequence=1|fechaarchivo=1 de enero de 2015}}</ref> que describen los objetos y las relaciones entre ellos, y pueden acabar siendo aceptados por convención. Como ejemplos tenemos los archivos tipo [[hojas de cálculo]], [[HTML]], [[XML]] o [[JSON]].

Un informe de 2011 del [[McKinsey Global Institute]] caracteriza los componentes principales y el ecosistema de big data de la siguiente manera:<ref>{{Cita web|url=https://www.mckinsey.com/business-functions/digital-mckinsey/our-insights/big-data-the-next-frontier-for-innovation|título=Big data: The next frontier for innovation, competition, and productivity|autor=James Manyika, Michael Chui, Brad Brown, Jacques Bughin, Richard Dobbs, Charles Roxburgh, and Angela Hung Byers|año=2011|editorial=McKinsey&Company}}</ref>

* Técnicas para analizar datos, como [[Test A/B|pruebas A / B]], [[aprendizaje automático]] y [[Procesamiento de lenguajes naturales|procesamiento del lenguaje natural]]
* Grandes tecnologías de datos, como [[Inteligencia empresarial|inteligencia de negocios]], [[computación en la nube]] y bases de datos
* Visualización, como tablas, gráficos y otras visualizaciones de los datos

Los big data multidimensionales también se pueden representar como cubos de datos o, matemáticamente, tensores. Los sistemas de bases de datos Array<nowiki/> se han propuesto proporcionar almacenamiento y soporte de consultas de alto nivel en este tipo de datos. Las tecnologías adicionales que se aplican a big data incluyen un cálculo basado en tensor eficiente,<ref>{{Cita web|url=http://www.cs.cornell.edu/cv/tenwork/finalreport.pdf|título="Future Directions in Tensor-Based Computation and Modeling"|año=2009}}</ref> como el aprendizaje de subespacio multilineal,<ref>{{Cita web|url=http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf|título=A Survey of Multilinear Subspace Learning for Tensor Data|autor=Haiping Lu, K. N. Plataniotis, A. N. Venetsanopoulos|año=2011|editorial=Pattern Recognition}}</ref> bases de datos de procesamiento paralelo masivo (MPP), [[Search based applications|aplicaciones basadas en búsqueda]], [[Minería de datos|extracción de datos]],<ref>{{Cita web|url=https://ieeexplore.ieee.org/document/6041580/?reload=true&arnumber=6041580|título=A Survey of the State of the Art in Data Mining and Integration Query Languages|autor=Pllana, Sabri; Janciak, Ivan; Brezany, Peter; Wöhrer, Alexander|año=2011|editorial=International Conference on Network-Based Information Systems (NBIS 2011). Computer Society}}</ref> [[sistemas de archivos distribuidos]], [[Base de datos distribuida|bases de datos distribuidas]], [[Computación en la nube|nube]]<nowiki/> e infraestructura basada en [[Supercomputadora|HPC]](aplicaciones, almacenamiento y recursos informáticos)<ref>{{Cita web|url=https://ieeexplore.ieee.org/document/6877311|título=Characterization and Optimization of Memory-Resident MapReduce on HPC Systems|autor=Yandong Wang ; Robin Goldstone ; Weikuan Yu ; Teng Wang|año=2014|editorial=IEEE}}</ref> e Internet. A pesar de que se han desarrollado muchos enfoques y tecnologías, sigue siendo difícil llevar a cabo el aprendizaje automático con grandes datos.<ref>{{Cita web|url=https://ieeexplore.ieee.org/document/7906512|título=Machine Learning With Big Data: Challenges and Approaches - IEEE Journals & Magazine|autor=L’Heureux, A.; Grolinger, K.; Elyamany, H. F.; Capretz, M. A. M.|año=2017|sitioweb=ieeexplore.ieee.org|idioma=en-US}}</ref>

Algunas bases de datos relacionales de MPP tienen la capacidad de almacenar y administrar petabytes de datos. Implícita es la capacidad de cargar, supervisar, realizar copias de seguridad y optimizar el uso de las tablas de datos de gran tamaño en el [[Sistema de gestión de bases de datos relacionales|RDBMS]].<ref>{{Cita web|url=http://www.dbms2.com/2009/04/30/ebays-two-enormous-data-warehouses/|título=eBay's two enormous data warehouses {{!}} DBMS&nbsp;2 : DataBase Management System Services|autor=Monash, Curt|año=2009|sitioweb=www.dbms2.com}}</ref>

El programa de [[Análisis Topológico de Datos]]<nowiki/> de [[Agencia de Proyectos de Investigación Avanzados de Defensa| DARPA ]]<nowiki/> busca la estructura fundamental de los conjuntos de datos masivos y en 2008 la tecnología se hizo pública con el lanzamiento de una compañía llamada Ayasdi.<ref>{{Cita web|url=https://www.ayasdi.com/resources/|título="Resources on how Topological Data Analysis is used to analyze big data"|sitioweb=Ayasdi|idioma=en-US}}</ref>

Los profesionales de los procesos de análisis de big data generalmente son hostiles al almacenamiento compartido más lento,<ref>{{Cita noticia|autor=John Webster|título=Storage area networks need not apply|url=https://www.cnet.com/news/storage-area-networks-need-not-apply/|fecha=1 de abril de 2011|periódico=CNET|idioma=en}}</ref> prefieren el almacenamiento de conexión directa (DAS) en sus diversas formas, desde [[unidad de estado sólido]] (SSD) hasta disco SATA de gran capacidad enterrado dentro de nodos de procesamiento paralelo. La percepción de las arquitecturas de almacenamiento compartidas, la red de área de almacenamiento ([[Red de área de almacenamiento|SAN]]) y el almacenamiento conectado a la red ([[Almacenamiento conectado en red|NAS]]), es que son relativamente lentas, complejas y costosas. Estas cualidades no son consistentes con los sistemas de análisis de datos grandes que prosperan en el rendimiento del sistema, infraestructura de productos básicos y bajo costo.

La entrega de información real o casi en tiempo real es una de las características definitorias del análisis de big data. Por lo tanto, se evita la latencia siempre que sea posible. Los datos en la memoria son buenos; los datos en el disco giratorio en el otro extremo de una conexión [[Fibra óptica|FC]] SAN no lo son. El costo de una SAN en la escala necesaria para las aplicaciones analíticas es mucho mayor que otras técnicas de almacenamiento.

Hay ventajas y desventajas para el almacenamiento compartido en el análisis de big data, pero los practicantes de análisis de big data a partir de 2011 no lo favorecieron.

=== Captura ===
¿De dónde provienen todos estos datos? Los fabricamos directa e indirectamente segundo tras segundo. Un [[iPhone]] hoy en día tiene más capacidad de cómputo que la [[NASA]] cuando el ser humano llegó a la Luna,<ref>{{cita publicación| apellidos1=Paniagua|nombre1=Soraya|título=A world of sensors, from Data to Big Data|publicación=Revista Telos|fecha=Junio - septiembre 2013}}</ref> por lo que la cantidad de datos generados por persona y en unidad de tiempo es muy grande. Catalogamos la procedencia de los datos según las siguientes categorías:<ref>“Conceptos básicos de Big Data”, TRC Informática SL, 2013.</ref>

* Generados por las propias personas. El hecho de enviar correos electrónicos o mensajes por [[WhatsApp]], publicar un estado en [[Facebook]], publicar relaciones laborales en [[Linkedin]], [[twitter|tuitear]] contenidos o responder a una encuesta por la calle son cosas que hacemos a diario y que crean nuevos datos y [[metadatos]] que pueden ser analizados. Se estima que cada minuto al día se envían más de 200 millones de [[e-mail|correos electrónicos]], se comparten más de 700 000 piezas de contenido en [[Facebook]], se realizan dos millones de búsquedas en Google o se editan 48 horas de vídeo en YouTube.<ref>{{cita publicación|apellidos1=Paniagua|nombre1=Soraya|título=A world of sensors, from Data to Big Data|publicación=Revista Telos|fecha=junio-septiembre 2013}}</ref> Por otro lado, las trazas de utilización en un sistema [[Sistema de planificación de recursos empresariales|ERP]], incluir registros en una [[base de datos]] o introducir información en una [[hoja de cálculo]] son otras formas de generar estos datos.
* Obtenidas a partir de transacciones. La facturación, tarjetas de fidelización, las llamadas telefónicas, las conexiones torres de telefonía, los accesos a wifis públicas, el pago con tarjetas de crédito o las transacciones entre cuentas bancarias generan información que tratada puede ser datos relevantes. Por ejemplo transacciones bancarias: Lo que el usuario conoce como un ingreso de X euros, el sistema lo capturará como una acción llevada a cabo en una fecha y momento determinado, en un lugar concreto, entre unos usuarios registrados, y con ciertos [[metadatos]].
* Mercadotecnia electrónica y web. Se genera una gran cantidad de datos cuando se navega por [[internet]]. Con la [[web]] 2.0 se ha roto el [[paradigma]] ''[[webmaster]]''-contenido-lector y los mismos usuarios se convierten en creadores de contenido gracias a su interacción con el sitio. Existen muchas herramientas de seguimiento utilizadas en su mayoría con fines de [[mercadotecnia]] y [[análisis de negocio]]. Los movimientos de ratón quedan grabados en [[mapa de calor|mapas de calor]] y queda registro de cuánto pasamos en cada página y cuándo las visitamos.
* Obtenidos a partir de las interacciones máquina a máquina ([[M2M]]). Son datos obtenidos a partir de la recogida de métricas obtenidas desde dispositivos (medidores, [[sensor]]es de temperatura, de luz, de altura, de presión, de sonido…) que transforman las magnitudes físicas o químicas y las convierten en datos. Existen desde hace décadas, pero la llegada de las comunicaciones inalámbricas ([[wifi]], [[Bluetooth]], [[RFID]], etc.) ha revolucionado el mundo de los sensores. Algunos ejemplos son los [[GPS]] en la automoción, los sensores de signos vitales (muy útil para seguros de vida), pulseras en los festivales,<ref>[http://www.theboxpopuli.com/blog/big-data-festivales-musica/ Big Data en los Festivales de Música] {{Wayback|url=http://www.theboxpopuli.com/blog/big-data-festivales-musica/ |date=20180317112706 }}. The Box Populi. 8 de Marzo 2018</ref> monitorizadores del funcionamiento y conducción de autoḿoviles (se obtiene información muy útil para las aseguradoras),<ref>[http://blog.segurostv.es/tecnologia-iot-big-data-futuro-del-sector-asegurador/ Tecnología IoT y big data: el futuro del sector asegurador]. 15 de febrero de 2018</ref> los smartphone (son sensores de localización).
* Datos biométricos recolectados. En general provienen de servicios de seguridad, defensa y servicios de inteligencia.<ref>{{cita publicación| apellidos1=Kohlwey|nombre1=Edmund|apellidos2=Sussman|nombre2=Abel|apellidos3=Trost|nombre3=Jason|apellidos4=Maurer| nombre4=Amber|título=Leveraging the Cloud for Big Data Biometrics|publicación=IEEE World Congress on Services| fecha=2011}}</ref> Son cantidades de datos generados por [[lector biométrico|lectores biométricos]] como escáneres de retina, escáneres de [[huella digital|huellas digitales]], o lectores de cadenas de [[ADN]]. El propósito de estos datos es proporcionar mecanismos de seguridad y suelen estar custodiados por los ministerios de defensa y departamentos de inteligencia. Un ejemplo de aplicación es el cruce de [[ADN]] entre una muestra de un crimen y una muestra en nuestra [[base de datos]].

=== Transformación ===
Una vez encontradas las fuentes de los datos necesarios, muy posiblemente dispongamos de un sinfín de tablas de origen que no estarán relacionadas. El siguiente objetivo es hacer que los datos se recojan en un mismo lugar y darles un formato adecuado. 

Aquí entran en juego las plataformas [[extract, transform and load|extraer, transformar y cargar]] (ETL). Su propósito es extraer los datos de las diferentes fuentes y sistemas, para después hacer transformaciones (conversiones de datos, limpieza de [[dirty data|datos sucios]], cambios de formato, etc.) y finalmente cargar los datos en la base de datos o [[Data Warehouse|almacén de datos]] especificada.<ref>{{cita publicación|apellidos1=Tomsen|nombre1=Christian|apellidos2=Pedersen|nombre2=Torben Bach|título=pygrametl: A Powerful Programming Framework for Extract–Transform–Load Programmers|publicación=1DB Technical Report; No. 25,  Department of Computer Science, Aalborg University|fecha=2009|url=http://vbn.aau.dk/files/18915819/dbtr-25.pdf}}</ref> Un ejemplo de plataforma ETL es el [[Pentaho]] Data Integration, más concretamente su aplicación ''[[Spoon]]''.

=== Almacenamiento NoSQL ===
El término [[NoSQL]] se refiere a ''Not Only'' ''SQL'' (''no solo SQL'') y son sistemas de almacenamiento que no cumplen con el esquema entidad-relación.<ref>{{cita publicación|apellidos1=Martín|nombre1=Adriana|apellidos2=Chávez|nombre2=Susana|apellidos3=Rodríguez|nombre3=Nelson R.|apellidos4=Valenzuela|nombre4=Adriana|apellidos5=Murazzo|nombre5=Maria A.|título=Bases de datos NoSql en cloud computing|publicación=WICC|fecha=2013|url=http://sedici.unlp.edu.ar/handle/10915/27121|fechaacceso=18 de octubre de 2014}}</ref> Proveen un sistema de almacenamiento mucho más flexible y concurrente y permiten manipular grandes cantidades de información de manera mucho más rápida que las [[base de datos|bases de datos relacionales]].

Distinguimos cuatro grandes grupos de bases de datos [[NoSQL]]:

* Almacenamiento clave-valor (''key-value''): los datos se almacenan de forma similar a los mapas o [[diccionario de datos|diccionarios de datos]], donde se accede al dato a partir de una clave única.<ref name="survey">{{cita publicación|apellidos1=Hecht|nombre1=Robin|apellidos2=Jablonski|nombre2=Stefan|título=NoSQL Evaluation, a use case oriented survey|publicación=International Conference on Cloud and Service Computing|fecha=2011|url=http://rogerking.me/wp-content/uploads/2012/03/DatabaseSystemsPaper.pdf}}</ref> Los valores (datos) son aislados e independientes entre ellos, y no son interpretados por el sistema. Pueden ser [[Variable (programación)|variable]]s simples como [[enteros]] o caracteres, u [[OOP|objetos]]. Por otro lado, este sistema de almacenamiento carece de una estructura de datos clara y establecida, por lo que no requiere un formateo de los datos muy estricto.<ref>{{cita publicación|apellidos1=Seeger|nombre1=Marc|título=Key-Value stores: a practical overview|fecha=21 de septiembre de 2009|url=http://d2tyy2n2j2cu1h.cloudfront.net/assets/papers/Ultra_Large_Sites_SS09-Seeger_Key_Value_Stores.pdf|fechaacceso=1 de enero de 2015}}</ref>

Son útiles para operaciones simples basadas en las claves. Un ejemplo es el aumento de velocidad de carga de un sitio [[web]] que puede utilizar diferentes perfiles de [[usuario]], teniendo mapeados los archivos que hay que incluir según el id de usuario y que han sido calculados con anterioridad. [[Apache Cassandra]] es la tecnología de almacenamiento clave-valor más reconocida por los usuarios.<ref name="Bianchi">{{cita publicación|apellidos1=Bianchi Widder|nombre1=Maria Belén|título=Els beneficis de l’ús de tecnologies NoSQL|publicación=UPCommons|fecha=septiembre de 2012|url=http://upcommons.upc.edu/pfc/bitstream/2099.1/16122/1/85121.pdf|fechaacceso=1 de enero de 2015}}</ref>
* Almacenamiento documental: las [[Base de datos documental|bases de datos documentales]] guardan un gran parecido con las bases de datos Clave-Valor, diferenciándose en el dato que guardan. Si en el anterior no se requería una [[estructura de datos]] concreta, en este caso guardamos datos semiestructurados.<ref name="Bianchi" /> Estos datos pasan a llamarse documentos, y pueden estar formateados en [[XML]], [[JSON]], [[BSON|Binary JSON]] o el que acepte la misma [[base de datos]].
:Todos los documentos tienen una clave única con la que pueden ser accedidos e identificados explícitamente. Estos documentos no son opacos al sistema, por lo que pueden ser interpretados y lanzar queries sobre ellos.<ref name="survey" /> Un ejemplo que aclare cómo se usa lo encontramos en un [[blog]]: se almacena el autor, la fecha, el título, el resumen y el contenido del post.
[[CouchDB]] o [[MongoDB]]<ref name="Bianchi" /> son quizá las más conocidas. Hay que hacer mención especial a [[MapReduce]], una tecnología de [[Google]] inicialmente diseñada para su algoritmo [[PageRank]], que permite seleccionar un subconjunto de datos, agruparlos o reducirlos y cargarlos en otra colección, y a [[Hadoop]] que es una tecnología de [[Servidor HTTP Apache|Apache]] diseñada para almacenar y procesar grandes cantidades de datos.
* Almacenamiento en grafo: las [[Base de datos orientada a grafos|bases de datos en grafo]] rompen con la idea de tablas y se basan en la [[teoría de grafos]], donde se establece que la información son los nodos y las relaciones entre la información son las aristas,<ref name="Bianchi" /> algo similar al [[modelo relacional]]. Su mayor uso se contempla en casos de relacionar grandes cantidades de datos que pueden ser muy variables. Por ejemplo, los [[Nodo (informática)|nodos]] pueden contener [[Objeto (programación)|objetos]], [[Variable (programación)|variables]] y [[Atributo (informática)|atributos]] diferentes en unos y otros. Las operaciones [[Sentencia JOIN en SQL|JOIN]] se sustituyen por recorridos a través del grafo, y se guarda una [[lista de adyacencia]]s entre los nodos.<ref name="survey" /> Encontramos un ejemplo en las redes sociales: en [[Facebook]] cada nodo se considera un [[usuario]], que puede tener [[Arista (teoría de grafos)|aristas]] de amistad con otros usuarios, o [[Arista (teoría de grafos)|aristas]] de publicación con [[Nodo (informática)|nodos]] de contenidos. Soluciones como [[Neo4J]] y GraphDB<ref name="Bianchi" /> son las más conocidas dentro de las [[Base de datos orientada a grafos|bases de datos en grafo]].
* Almacenamiento orientado a columnas: por último, este almacenamiento es parecido al [[Base de datos documental|documental]]. Su modelo de datos es definido como «un mapa de datos [[multidimensional]] poco denso, distribuido y persistente».<ref name="survey" /> Se orienta a almacenar datos con tendencia a escalar horizontalmente, por lo que permite guardar diferentes [[Atributo (informática)|atributos]] y [[Objeto (programación)|objetos]] bajo una misma clave. A diferencia del [[Base de datos documental|documental]] y el clave-valor, en este caso se pueden almacenar varios [[Atributo (informática)|atributos]] y [[Objeto (programación)|objetos]], pero no serán interpretables directamente por el sistema. Permite agrupar columnas en familias y guardar la información cronológicamente, mejorando el rendimiento. Esta tecnología se acostumbra a usar en casos con 100 o más atributos por clave.<ref name="Bianchi" /> Su precursor es [[BigTable]] de Google, pero han aparecido nuevas soluciones como HBase o HyperTable.

=== Análisis de datos ===
El análisis permite mirar los datos y explicar lo que esta pasando. Teniendo los datos necesarios almacenados según diferentes tecnologías de [[dispositivo de almacenamiento de datos|almacenamiento]], nos daremos cuenta que necesitaremos diferentes técnicas de [[análisis de datos]] como las siguientes:

* Asociación: permite encontrar relaciones entre diferentes variables.<ref>{{cita publicación|apellidos1=Vila|nombre1=M Amparo|apellidos2=Sanchez|nombre2=Daniel|apellidos3=Escobar|nombre3=Luis|título=Relaciones Causales en Reglas de Asociación|publicación=XII Congreso Español sobre tecnologías y lógica Fuzzy|fecha=2004|url=http://decsai.ugr.es/~castro/docto-csi/LER/p44.pdf}}</ref> Bajo la premisa de causalidad, se pretende encontrar una predicción en el comportamiento de otras variables. Estas relaciones pueden ser los sistemas de [[Venta cruzada|ventas cruzadas]] en los [[e-commerce|comercios electrónicos]].
* Minería de datos (''[[data mining]]''): tiene como objetivo encontrar comportamientos predictivos. Engloba el conjunto de técnicas que combina métodos estadísticos y de [[aprendizaje automático]] con almacenamiento en bases de datos.<ref name="Manyika">{{cita publicación|apellidos1=Manyika|nombre1=James|apellidos2=Chui|nombre2=Michael|apellidos3=Brown|nombre3=Brad|apellidos4=Bughin|nombre4=Jacques|apellidos5=Dobbs|nombre5=Richard|apellidos6=Roxburgh|nombre6=Charles|apellidos7=Byers|nombre7=Angela Hung|título=Big data: The next frontier for innovation, competition, and productivity|publicación=McKinsey|fecha=Mayo de 2011|url=http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation|fechaacceso=1 de enero de 2015}}</ref> Está estrechamente relacionada con los [[Modelo estadístico|modelos]] utilizados para descubrir patrones en grandes cantidades de datos.
*Agrupación (''[[clustering]]''): el análisis de clústeres es un tipo de [[Data Mining|minería de datos]] que divide grandes grupos de individuos en grupos más pequeños de los cuales no conocíamos su parecido antes del análisis.<ref name="Manyika" /> El propósito es encontrar similitudes entre estos grupos, y el descubrimiento de nuevos, conociendo cuáles son las cualidades que lo definen. Es una metodología apropiada para encontrar relaciones entre resultados y hacer una evaluación preliminar de la estructura de los datos analizados. Existen diferentes técnicas y algoritmos de clusterización.<ref>{{cita publicación|apellidos1=Jain|nombre1=A.K.|apellidos2=Murty|nombre2=M.N.|apellidos3=Flynn|nombre3=P.J.|título=Data Clustering: A Review|publicación=ACM Computing Surveys|fecha=septiembre de 1999|volumen=31|número=3|url=https://ai.vub.ac.be/sites/default/files/dataclustering.pdf|fechaacceso=1 de enero de 2015}}</ref>
* Análisis de texto (''[[Minería de textos|text analytics]]''): gran parte de los datos generados por las personas son textos, como [[Correo electrónico|correos]], búsquedas [[web]] o contenidos. Esta metodología permite extraer información de estos datos y así modelar temas y asuntos o predecir palabras.<ref>{{cita publicación|apellidos1=Maltby|nombre1=Dylan|título=Big Data Analytics|publicación=ASIST 2011|fecha=9 de octubre de 2011|url=https://www.ischool.utexas.edu/~dmaltby/Big_Data_Analytics.pdf|ubicación=New Orleans|fechaacceso=1 de enero de 2015|urlarchivo=https://web.archive.org/web/20150101204208/https://www.ischool.utexas.edu/~dmaltby/Big_Data_Analytics.pdf|fechaarchivo=1 de enero de 2015}}</ref>

=== Visualización de datos ===
[[Archivo:Infografia NucleoLinux.png|300px|miniaturadeimagen|derecha|Esto es una [[infografía]].]]

Tal y como el [[Instituto Nacional de Estadística (España)|Instituto Nacional de Estadística]] dice en sus tutoriales, «una imagen vale más que mil palabras o que mil datos».<ref>{{Cita web|url=https://www.ine.es/explica/docs/pasos_estadistica_atractiva.pdf|título=Primeros pasos / Información más atractiva}}</ref> La mente agradece mucho más una presentación bien estructurada de resultados estadísticos en gráficos o mapas en vez de en tablas con números y conclusiones. En los macrodatos se llega un paso más allá: parafraseando a [[Edward Tufte]], uno de los expertos en visualización de datos más reconocidos a nivel mundial «el mundo es complejo, dinámico, multidimensional, el papel es estático y plano. ¿Cómo vamos a representar la rica experiencia visual del mundo en la mera planicie?».

[[Mondrian (informática)|Mondrian]]<ref>{{cita publicación|apellidos1=Theus|nombre1=Martin|título=Interactive Data Visualization using Mondrian|publicación=Journal of Statistical Software|fecha=2003}}</ref> es una plataforma que permite visualizar la información a través de los análisis llevados a cabo sobre los datos que disponemos. Con esta plataforma se intenta llegar a un público más concreto, y una utilidad más acotada como un cuadro de mando integral de una organización. En los últimos años se han generalizado otras plataformas como Tableau, Power BI y Qlik.<ref>{{Cita noticia|título=Tableau vs Qlikview {{!}} Tableau vs Power BI {{!}} Power BI vs Qlikview - 2018|url=https://selecthub.com/business-intelligence/tableau-vs-qlikview-vs-microsoft-power-bi/|fecha=27 de agosto de 2018|fechaacceso=16 de octubre de 2018|periódico=SelectHub|idioma=en-US}}</ref> 

Por otro lado, las [[infografía]]s se han vuelto un fenómeno viral, donde se recogen los resultados de los diferentes análisis sobre nuestros datos, y son un material atractivo, entretenido y simplificado para audiencias masivas.<ref>{{cita publicación|apellidos1=Albarracín|nombre1=Pablo|título=Visualización avanzada de datos: La belleza del Big Data|publicación=Revista América Economía Tecno|fecha=12 de agosto de 2013|url=http://tecno.americaeconomia.com/noticias/visualizacion-avanzada-de-datos-la-belleza-del-big-data|fechaacceso=18 de octubre de 2014|urlarchivo=https://web.archive.org/web/20150101224955/http://tecno.americaeconomia.com/noticias/visualizacion-avanzada-de-datos-la-belleza-del-big-data|fechaarchivo=1 de enero de 2015}}</ref>

== Aplicaciones ==
El uso de big data ha sido utilizado por la industria de los medios, las empresas y los gobiernos para dirigirse con mayor precisión a su público y aumentar la eficiencia de sus mensajes. 

El big data ha aumentado la demanda de especialistas en administración de la información tanto que [[Software AG]], [[Oracle Corporation]], [[IBM]], [[Microsoft]], [[SAP SE|SAP]], [[Dell EMC|EMC]], [[HP Inc.|HP]] y [[Dell]] han gastado más de $ 15 mil millones en firmas de software especializadas en administración y análisis de datos. En 2010, esta industria valía más de $ 100 mil millones y crecía a casi un 10 por ciento anual: aproximadamente el doble de rápido que el negocio del software en general.<ref>{{Cita noticia|título=Data, data everywhere|url=https://www.economist.com/special-report/2010/02/25/data-data-everywhere|fecha=25 de febrero de 2010|periódico=The Economist|idioma=en}}</ref> 

Las economías desarrolladas usan cada vez más tecnologías intensivas en datos. Hay 4600 millones de suscripciones de teléfonos móviles en todo el mundo, y entre 1000 y 2000 millones de personas que acceden a Internet. Entre 1990 y 2005, más de mil millones de personas en todo el mundo ingresaron a la clase media, lo que significa que más personas se volvieron más alfabetizadas, lo que a su vez llevó al crecimiento de la información. La capacidad efectiva mundial para intercambiar información a través de redes de telecomunicaciones era de 281 [[Petabyte|petabytes]] en 1986, 471 petabytes en 1993, 2.2 exabytes en 2000, 65 [[Exabyte|exabytes]] en 2007<ref>{{Cita noticia|autor=Hilbert, Martin; López, Priscila|título=The World’s Technological Capacity to Store, Communicate, and Compute Information|url=http://www.martinhilbert.net/WorldInfoCapacity.html/|periódico=MartinHilbert.net|idioma=en-US}}</ref> y las predicciones cifran el tráfico de internet en 667 exabytes anualmente para 2014. Según una estimación, un tercio de la información almacenada en todo el mundo está en forma de texto alfanumérico e imágenes fijas,<ref>{{Cita publicación|url=https://www.tandfonline.com/doi/abs/10.1080/01972243.2013.873748|título=What Is the Content of the World's Technologically Mediated Information and Communication Capacity: How Much Text, Image, Audio, and Video?|apellidos=Hilbert|nombre=Martin|fecha=2014-03|publicación=The Information Society|volumen=30|número=2|páginas=127–143|idioma=en|issn=0197-2243|doi=10.1080/01972243.2013.873748}}</ref> que es el formato más útil para la mayoría de las aplicaciones de big data. Esto también muestra el potencial de los datos aún no utilizados (es decir, en forma de contenido de video y audio).

Si bien muchos proveedores ofrecen soluciones estándar para big data, los expertos recomiendan el desarrollo de soluciones internas personalizadas para resolver el problema de la compañía si la empresa cuenta con capacidades técnicas suficientes.<ref>{{Cita web|url=https://www.kdnuggets.com/2014/07/interview-amy-gershkoff-ebay-in-house-BI-tools.html|título=Interview: Amy Gershkoff, Director of Customer Analytics & Insights, eBay on How to Design Custom In-House BI Tools|autor=Rajpurohit, Anmol|fecha=11 de julio de 2014|sitioweb=www.kdnuggets.com|idioma=en-US}}</ref>

=== Gobierno ===
{{AP|Gobierno por algoritmos}}

[[Archivo:AlgorithmicGovernance.svg|miniaturadeimagen|La aplicación del Big Data en el sistema jurídico, junto a técnicas de análisis, es considerada en la actualidad como una de las vías posibles para agilizar la administración de justicia.]]

El uso y la adopción de big data dentro de los procesos gubernamentales permite eficiencias en términos de costo, productividad e innovación, pero no viene sin sus defectos.<ref>{{Cita noticia|apellidos=Davis|nombre=Aaron|título=The government and big data: Use, problems and potential|url=https://www.computerworld.com/article/2472667/government-it/the-government-and-big-data--use--problems-and-potential.html|fechaacceso=27 de agosto de 2018|periódico=Computerworld|idioma=en}}</ref> El análisis de datos a menudo requiere que varias partes del gobierno (central y local) trabajen en colaboración y creen procesos nuevos para lograr el resultado deseado.

Los datos masivos se usan habitualmente para influenciar el proceso democrático. Los representantes del pueblo pueden ver todo lo que hacen los ciudadanos, y los ciudadanos pueden dictar la vida pública de los representantes mediante tuits y otros métodos de extender ideas en la sociedad. Las campañas presidenciales de [[Barack Obama|Obama]] y [[Donald Trump|Trump]] los usaron de manera generalizada<ref name="theclinic.cl">Martin Hilbert, experto en redes digitales: “Obama y Trump usaron el Big Data para lavar cerebros” http://www.theclinic.cl/2017/01/19/martin-hilbert-experto-redes-digitales-obama-trump-usaron-big-data-lavar-cerebros/</ref> y hay expertos que advierten de que hay que «reinventar la democracia representativa. Si no, es posible que se convierta en una dictadura de la información».<ref>{{Cita noticia|apellidos=Lissardy|nombre=Gerardo|título=Martin Hilbert, gurú del Big Data: "La democracia no está preparada para la era digital y está siendo destruida"|url=https://www.bbc.co.uk/mundo/noticias-internacional-39511606|fecha=6 de abril de 2017|fechaacceso=27 de agosto de 2018|periódico=BBC News Mundo|idioma=en-GB}}</ref>

El [[Banco Interamericano de Desarrollo]] (BID) ha desarrollado estudios en América Latina en los que presenta distintos casos del uso de Macrodatos en el diseño e implementación de [[políticas públicas]]. Destacando intervenciones en temas de movilidad urbana, ciudades inteligentes y seguridad, entre otras temáticas. Las recomendaciones de los mismos han girado en torno a cómo construir instituciones públicas que logren, mediante el uso de datos masivos, a ser más transparentes y ayuden a tomar mejores decisiones.<ref>{{Cita noticia|apellidos=Rodríguez. Patricio / Palomino. Norma/ Moncada. Javier |título=El uso de datos masivos y sus técnicas analíticas para el diseño e implementación de políticas públicas en Latinoamérica y el Caribe (2017)|url=https://publications.iadb.org/handle/11319/8485|fecha=Julio de 2017|fechaacceso=29 de noviembre de 2018|periódico=BID|idioma=ES}}</ref>

=== Desarrollo internacional ===
La investigación sobre el uso efectivo de las [[Tecnologías de la información y la comunicación|tecnologías de información y comunicación]] para el desarrollo (también conocido como [[ICT4D]]) sugiere que la tecnología de big data puede hacer contribuciones importantes pero también presentar desafíos únicos para el desarrollo internacional.<ref>{{Cita web|url=https://www.unglobalpulse.org/projects/BigDataforDevelopment|título=White Paper: Big Data for Development: Opportunities & Challenges (2012) {{!}} United Nations Global Pulse|fechaacceso=27 de agosto de 2018|sitioweb=www.unglobalpulse.org|idioma=en}}</ref><ref>{{Cita web|url=https://www.weforum.org/reports/big-data-big-impact-new-possibilities-international-development|título=Big Data, Big Impact: New Possibilities for International Development|fechaacceso=27 de agosto de 2018|sitioweb=World Economic Forum}}</ref> Los avances en el análisis de big data ofrecen oportunidades rentables para mejorar la [[toma de decisiones]] en áreas de desarrollo críticas como la atención médica, el empleo, la productividad económica, la delincuencia, la seguridad y el manejo de recursos y desastres naturales.<ref name=":0">{{Cita publicación|url=https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2205145|título=Big Data for Development: From Information - to Knowledge Societies|apellidos=Hilbert|nombre=Martin|fecha=2013|publicación=SSRN Electronic Journal|fechaacceso=27 de agosto de 2018|idioma=en|issn=1556-5068|doi=10.2139/ssrn.2205145}}</ref> Además, los datos generados por el usuario ofrecen nuevas oportunidades para ofrecer una voz inaudita. Sin embargo, los desafíos de larga data para las regiones en desarrollo, como la infraestructura tecnológica inadecuada y la escasez de recursos económicos y humanos exacerban las preocupaciones existentes con los grandes datos, como la [[privacidad]], la metodología imperfecta y los problemas de [[interoperabilidad]].<ref name=":0" />

=== Industria ===
El big data proporciona una infraestructura para la transparencia en la industria manufacturera, que es la capacidad de desentrañar incertidumbres como el rendimiento y la disponibilidad de componentes inconsistentes. La [[fabricación predictiva]] como un enfoque aplicable para el tiempo de inactividad y la transparencia cercanos a cero requiere una gran cantidad de datos y herramientas de predicción avanzadas para un proceso sistemático de datos en información útil.<ref>{{Cita publicación|url=https://www.sciencedirect.com/science/article/pii/S0888327013002860|título=Prognostics and health management design for rotary machinery systems—Reviews, methodology and applications|fecha=1 de enero de 2014|publicación=Mechanical Systems and Signal Processing|volumen=42|número=1-2|páginas=314–334|fechaacceso=27 de agosto de 2018|idioma=en|issn=0888-3270|doi=10.1016/j.ymssp.2013.06.004}}</ref> Un marco conceptual de fabricación predictiva comienza con la adquisición de datos donde se encuentran disponibles diferentes tipos de datos sensoriales, tales como acústica, vibración, presión, corriente, voltaje y datos de controlador. Una gran cantidad de datos sensoriales, además de los datos históricos, construyen los grandes datos en la fabricación. Los big data generados actúan como la entrada en herramientas predictivas y estrategias preventivas como Pronósticos y Gestión de Salud (PHM).<ref>{{Cita web|url=https://www.itri.org.tw/eng/Content/MSGPic01/contents.aspx?&SiteID=1&MmmID=620651706136357202&CatID=620653256103620163&MSID=654532365564567545|título=Prognostic and Health Management Technology for MOCVD Equipment|fechaacceso=27 de agosto de 2018|sitioweb=Industrial Technology Research Institute|idioma=en|urlarchivo=https://web.archive.org/web/20180827075430/https://www.itri.org.tw/eng/Content/MSGPic01/contents.aspx?&SiteID=1&MmmID=620651706136357202&CatID=620653256103620163&MSID=654532365564567545|fechaarchivo=27 de agosto de 2018}}</ref>

=== Medios ===
Los profesionales en medios y publicidad abordan los grandes datos como muchos puntos de información procesables sobre millones de personas. La industria parece alejarse del enfoque tradicional de utilizar entornos de medios específicos, como periódicos, revistas o programas de televisión, y en su lugar aprovecha a los consumidores con tecnologías que llegan a las personas objetivo en momentos óptimos en ubicaciones óptimas. El objetivo final es servir o transmitir, un mensaje o contenido que (estadísticamente hablando) esté en línea con la mentalidad del consumidor. Por ejemplo, los entornos de publicación adaptan cada vez más los mensajes (anuncios publicitarios) y el contenido (artículos) para atraer a los consumidores que han sido recolectados exclusivamente a través de diversas actividades de extracción de datos.<ref>{{Cita web|url=http://eprints.lse.ac.uk/57944/|título=Advertising, big data and the clearance of the public realm: marketers' new approaches to the content subsidy|fechaacceso=27 de agosto de 2018|apellido=Nick|nombre=Couldry,|fecha=2014|sitioweb=eprints.lse.ac.uk|idioma=en}}</ref>

* Orientación de los consumidores (para publicidad de los vendedores)<ref>{{Cita noticia|título=Why Digital Advertising Agencies Suck at Acquisition and are in Dire Need of an AI Assisted Upgrade|url=https://ishti.org/2018/04/15/why-digital-advertising-agencies-suck-at-acquisition-and-are-in-dire-need-of-an-ai-assisted-upgrade/|fecha=15 de abril de 2018|fechaacceso=27 de agosto de 2018|periódico=Insincerely Yours|idioma=en-US}}</ref>
* [[Minería de datos]]
* [[Periodismo de datos]]: los editores y los periodistas usan herramientas de Big Data para proporcionar información e [[Infografía|infografías]] únicas e innovadoras.

=== Música ===
El reconocimiento de emociones musicales (REM) (Music Emotion Recognition MER) es un campo de investigación científica reciente y en plena evolución. A grandes rasgos se puede decir que el REM gira en torno a varias ideas respecto a la comprensión psicológica de la relación entre el afecto humano y la música. Una de las ideas centrales del REM radica en la capacidad de poder determinar mediante sistemas automáticos ingresando diversos datos (señales musicales) y variables (parámetros computacionales), cuáles y qué tipo de emociones son percibidas desde las composiciones musicales, e intentan percibir cómo cada una de las formas de sus rasgos estructurales pueden producir cierto tipo de reacciones características en los oyentes.<ref>{{cita web|apellidos1=Luján Villar|nombre1=Juan David|apellidos2=Luján Villar|nombre2=Roberto Carlos|título=Reconocimiento de emociones musicales a través de datos y tecnologías digitales|url=https://comunicacionyhombre.com/article/reconocimiento-emociones-musicales-traves-datos-tecnologias-digitales/|obra=COMUNICACIÓN Y HOMBRE|fechaacceso=12 de junio de 2020}}</ref>

=== Seguros ===
Los proveedores de seguro médico recopilan datos sobre "determinantes sociales", como el consumo de alimentos y televisión, el estado civil, el tamaño de la vestimenta y los hábitos de compra, desde los cuales hacen predicciones sobre los costos de salud para detectar problemas de salud en sus clientes. Es controvertido si estas predicciones se están utilizando actualmente para fijar precios.<ref>{{Cita web|url=https://www.propublica.org/article/health-insurers-are-vacuuming-up-details-about-you-and-it-could-raise-your-rates|título=Health Insurers Are Vacuuming Up Details About You — And It Could Raise Your Rates — ProPublica|fechaacceso=27 de agosto de 2018|apellido=Allen|nombre=Marshall|fecha=17 de julio de 2018|sitioweb=ProPublica|idioma=en-us}}</ref>

=== Deportes ===
En un ámbito donde se mueve tanto dinero, suelen utilizar las nuevas tecnologías antes que los usuarios de base. Nos encontramos por ejemplo que el análisis de los partidos constituye una parte fundamental en el entrenamiento de los profesionales, y la toma de decisiones de los entrenadores.

[[Amisco]]<ref name="Futbol">{{cita libro|apellidos1=Reilly|nombre1=Thomas|apellidos2=Korkusuz|nombre2=Feza|título=Science and Football VI. The proceedings of the Sixth World Congress on Science and Football|fecha=2009|isbn=0-203-89368-9|página=209|url=http://wata.cc/up/2012/07/files/w-b6c9afb540.pdf#page=209|urlarchivo=https://web.archive.org/web/20150101224330/http://wata.cc/up/2012/07/files/w-b6c9afb540.pdf#page=209|fechaarchivo=1 de enero de 2015}}</ref> es un sistema aplicado por equipos de algunas de las ligas más importantes de Europa desde el 2001. Consta de 8 cámaras y diversos ordenadores instalados en los estadios, que registran los movimientos de los jugadores a razón de 25 registros por segundo, y luego envían los datos a una central donde hacen un [[minería de datos|análisis masivo de los datos]]. La información que se devuelve como resultado incluye una reproducción del partido en dos dimensiones, los datos técnicos y estadísticas, y un resumen de los datos físicos de cada jugador, permitiendo seleccionar varias dimensiones y visualizaciones diferentes de datos.<ref name="Futbol" />

=== Finanzas ===
El crecimiento de datos en el mundo financiero obliga al uso del big data para el procesamiento rápido de datos, gestión de la omnicanalidad, segmentación avanzada de clientes, creación de estrategias de precios dinámicos, gestión de riesgos, prevención de fraudes, apoyo en la toma de decisiones, detectar tendencias de consumo, definir nuevas formas de hacer mejor las cosas, detectar alertas y otro tipo de eventos complejos, hacer un seguimiento avanzado de la competencia.<ref>{{Cita web|url=https://www.datahack.es/big-data-finanzas/|título=Big Data y finanzas - datahack, especialistas en Big Data, más que una escuela y un máster|fechaacceso=16 de octubre de 2018|sitioweb=www.datahack.es|idioma=es-ES}}</ref>

=== Mercadotecnia y ventas ===
Los macrodatos cada vez se utilizan más para segmentación avanzada de los consumidores, automatizar la personalización de los productos, adaptar las comunicaciones al momento del ciclo de venta, captar nuevas oportunidades de venta, apoyo en la toma de decisiones a tiempo real, gestión de crisis.<ref>{{Cita web|url=https://www.datahack.es/big-data-marketing-ventas/|título=Por qué mezclar Big Data, Marketing y Ventas es una buena idea - datahack, especialistas en Big Data, más que una escuela y un máster|fechaacceso=16 de octubre de 2018|sitioweb=www.datahack.es|idioma=es-ES}}</ref><ref>{{Cita web|url=https://www.europapress.es/comunicados/empresas-00908/noticia-comunicado-euroinnova-presenta-nueva-formacion-sector-pleno-crecimiento-20190226180857.html|título=Big Data, una formación en crecimiento|fechaacceso=26 de febrero de 2019|autor=Europa Press}}</ref>

== Investigación ==
La búsqueda [[criptografía|encriptada]] y la formación de [[clúster (informática)|clúster]] en big data se demostraron en marzo de 2014 en la [[Sociedad Estadounidense de Educación en Ingeniería]]. [[Gautam Siwach]] participó en abordar los desafíos de Big Data por el [[MIT Computer Science and Artificial Intelligence Laboratory|Laboratorio de Ciencias de la Computación e Inteligencia Artificial]] del [[Instituto Tecnológico de Massachusetts|MIT]] y [[Amir Esmailpour]], en el Grupo de Investigación de UNH, investigó las características clave de Big Data como la formación de clusters y sus interconexiones. Se centraron en la seguridad de los macrodatos y la orientación del término hacia la presencia de diferentes tipos de datos en forma cifrada en la interfaz de la nube al proporcionar las definiciones sin procesar y los ejemplos de tiempo real dentro de la tecnología. Además, propusieron un enfoque para identificar la técnica de codificación para avanzar hacia una búsqueda acelerada sobre texto encriptado que conduzca a las mejoras de seguridad en big data.<ref>{{Cita publicación|url=http://asee-ne.org/proceedings/2014/Student%20Papers/210.pdf|título=Encrypted Search & Cluster Formation in Big Data|apellidos=Siwach|nombre=Gautam|apellidos2=Esmailpour|nombre2=Amir|fecha=2014|publicación=Department of Electrical and Computer Engineering
The University of New Haven|ubicación=West Haven, CT, USA|urlarchivo=https://web.archive.org/web/20140809045242/http://asee-ne.org/proceedings/2014/Student%20Papers/210.pdf|fechaarchivo=9 de agosto de 2014}}</ref>

En marzo de 2012, la Casa Blanca anunció una "Iniciativa de Big Data" nacional que consistía en seis departamentos y agencias federales comprometiendo más de $ 200 millones para proyectos de investigación de big data.

La iniciativa incluyó una subvención de la National Science Foundation "Expeditions in Computing" de $ 10 millones durante 5 años para el AMPLab<ref>{{Cita web|url=https://amplab.cs.berkeley.edu|título=AMPLab - UC Berkeley|fechaacceso=29 de septiembre de 2018|sitioweb=AMPLab - UC Berkeley|idioma=en-US}}</ref> en la [[Universidad de California]], Berkeley.<ref>{{Cita web|url=https://www.nsf.gov/news/news_summ.jsp?cntn_id=123607&org=NSF&from=news|título=NSF Leads Federal Efforts In Big Data {{!}} NSF - National Science Foundation|año=2012|sitioweb=www.nsf.gov|idioma=en}}</ref> El AMPLab también recibió fondos de [[Agencia de Proyectos de Investigación Avanzados de Defensa|DARPA]], y más de una docena de patrocinadores industriales y utiliza big data para atacar una amplia gama de problemas, desde predecir la congestión del tráfico<ref>{{Cita web|url=https://amplab.cs.berkeley.edu/publication/scaling-the-mobile-millennium-system-in-the-cloud-2/|título=Scaling the Mobile Millennium System in the Cloud|autor=Timothy Hunter, Teodor Moldovan, Matei Zaharia, Justin Ma, Michael Franklin, Pieter Abbeel, Alexandre Bayen|año=2011|sitioweb=AMPLab - UC Berkeley|idioma=en-US}}</ref> hasta combatir el cáncer.<ref>{{Cita noticia|autor=David Patterson|título=David Patterson: Enlist Computer Scientists in Cancer Fight|url=https://www.nytimes.com/2011/12/06/science/david-patterson-enlist-computer-scientists-in-cancer-fight.html?_r=0|fecha=5 de diciembre de 2011|periódico=The New York Times|idioma=en}}</ref>

La Iniciativa Big Data de la Casa Blanca también incluyó un compromiso del Departamento de Energía de proporcionar $ 25 millones en financiamiento durante 5 años para establecer el Instituto de Administración, Análisis y Visualización de Datos Escalables (SDAV),<ref>{{Cita web|url=https://www.energy.gov/articles/secretary-chu-announces-new-institute-help-scientists-improve-massive-data-set-research-doe|título=Secretary Chu Announces New Institute to Help Scientists Improve Massive Data Set Research on DOE Supercomputers|sitioweb=Energy.gov|idioma=en}}</ref> dirigido por [[Laboratorio Nacional Lawrence Berkeley|Lawrence Berkeley National Laboratory]] del Departamento de Energía. Laboratorio. El Instituto SDAV tiene como objetivo reunir la experiencia de seis laboratorios nacionales y siete universidades para desarrollar nuevas herramientas que ayuden a los científicos a gestionar y visualizar datos en las supercomputadoras del Departamento.

El estado de [[Massachusetts]] anunció la Iniciativa Big Data de Massachusetts en mayo de 2012, que proporciona fondos del gobierno estatal y de empresas privadas a una variedad de instituciones de investigación. El [[Instituto Tecnológico de Massachusetts|Instituto de Tecnología de Massachusetts]] alberga el Centro de Ciencia y Tecnología de Intel para Big Data en el [[MIT Computer Science and Artificial Intelligence Laboratory|Laboratorio de Ciencias de la Computación e Inteligencia Artificial del MIT]], que combina fondos y esfuerzos de investigación gubernamentales, corporativos e institucionales.<ref>{{Cita web|url=http://bigdata.csail.mit.edu|título=Welcome to Big Data at CSAIL {{!}} bigdata CSAIL|sitioweb=bigdata.csail.mit.edu|idioma=en}}</ref>

La Comisión Europea está financiando el Foro público privado de Big Data, que duró dos años, a través de su Séptimo Programa de Framework para involucrar a empresas, académicos y otras partes interesadas en la discusión de problemas de big data. El proyecto tiene como objetivo definir una estrategia en términos de investigación e innovación para guiar las acciones de apoyo de la [[Comisión Europea]] en la implementación exitosa de la economía de big data. Los resultados de este proyecto se utilizarán como aportación para [[Horizonte 2020]], su próximo programa.

El gobierno británico anunció en marzo de 2014 la fundación del [[Instituto Alan Turing]], que lleva el nombre del pionero de la informática y el descifrador de códigos, que se centrará en nuevas formas de recopilar y analizar grandes conjuntos de datos.<ref>{{Cita web|url=http://bigdata.csail.mit.edu|título=Welcome to Big Data at CSAIL {{!}} bigdata CSAIL|fecha=19 de marzo de 2014|sitioweb=bigdata.csail.mit.edu|idioma=en}}</ref>

En el Día de la Inspiración del Canadian Open Data Experience (CODE) de la [[Universidad de Waterloo Stratford Campus]], los participantes demostraron cómo el uso de la visualización de datos puede aumentar la comprensión y el atractivo de los grandes conjuntos de datos y comunicar su historia al mundo.<ref>{{Cita noticia|título=Inspiration day at University of Waterloo, Stratford Campus|url=https://betakit.com/event/inspiration-day-at-university-of-waterloo-stratford-campus/|fecha=28 de febrero de 2014|periódico=BetaKit|idioma=en-CA}}</ref>

Para que la fabricación sea más competitiva en los Estados Unidos (y en el mundo), es necesario integrar más ingenio e innovación estadounidenses en la fabricación; Por lo tanto, la National Science Foundation ha otorgado al centro de investigación cooperativa Industry Industry para Intelligent Maintenance Systems (IMS) en la [[Universidad de Cincinnati]] para que se concentre en el desarrollo de herramientas y técnicas predictivas avanzadas aplicables en un entorno de big data.<ref>{{Cita publicación|url=https://www.sciencedirect.com/science/article/pii/S2213846313000114|título=Recent advances and trends in predictive manufacturing systems in big data environment|apellidos=JayLee, Edzel Lapira, Behrad Bagheri, Hung-an Kao|fecha=1 de octubre de 2013|publicación=Manufacturing Letters|volumen=1|número=1|páginas=38–41|idioma=en|issn=2213-8463|doi=10.1016/j.mfglet.2013.09.005}}</ref> En mayo de 2013, el IMS Center celebró una reunión de la junta asesora de la industria centrada en big data, donde presentadores de varias compañías industriales discutieron sus preocupaciones, problemas y objetivos futuros en el entorno de big data.

Ciencias sociales computacionales: cualquier persona puede usar Interfaces de programación de aplicaciones (API) proporcionadas por grandes titulares de datos, como Google y Twitter, para realizar investigaciones en las ciencias sociales y del comportamiento.<ref>{{Cita web|url=http://www.ijis.net/ijis9_1/ijis9_1_editorial_pre.html|título=International Journal of Internet Science, Volume 9, Issue 1|sitioweb=www.ijis.net}}</ref> A menudo, estas API se proporcionan de forma gratuita. Tobias Preis usó los datos de [[Tendencias de búsqueda de Google|Tendencias de Google]] para demostrar que los usuarios de Internet de países con un producto interno bruto (PIB) per cápita más alto tienen más probabilidades de buscar información sobre el futuro que la información sobre el pasado. Los hallazgos sugieren que puede haber un vínculo entre el comportamiento en línea y los indicadores económicos del mundo real.<ref>{{Cita publicación|url=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3320057/|título=Quantifying the Advantage of Looking Forward|apellidos=Preis|nombre=Tobias|apellidos2=Moat|nombre2=Helen Susannah|fecha=5 de abril de 2012|publicación=Scientific Reports|volumen=2|fechaacceso=29 de septiembre de 2018|issn=2045-2322|doi=10.1038/srep00350|pmc=3320057|pmid=22482034|apellidos3=Stanley|nombre3=H. Eugene|apellidos4=Bishop|nombre4=Steven R.}}</ref><ref>{{Cita noticia|apellidos=Marks|nombre=Paul|título=Online searches for future linked to economic success|url=https://www.newscientist.com/article/dn21678|fecha=5 de abril de 2012|fechaacceso=29 de septiembre de 2018|periódico=New Scientist|idioma=en-US}}</ref><ref>{{Cita noticia|apellidos=Johnston|nombre=Casey|título=Google Trends reveals clues about the mentality of richer nations|url=https://arstechnica.com/gadgets/2012/04/google-trends-reveals-clues-about-the-mentality-of-richer-nations/|fecha=6 de abril de 2012|fechaacceso=29 de septiembre de 2018|periódico=Ars Technica|idioma=en-us}}</ref> Los autores del estudio examinaron los registros de consultas de [[Google]] realizados por la relación del volumen de búsquedas para el año siguiente ('2011') con el volumen de búsquedas del año anterior ('2009'), al que denominaron 'índice de orientación futura'.<ref>{{Cita web|url=http://tobiaspreis.de/bigdata/future_orientation_index.pdf|título=Supplementary Information: The Future Orientation Index is available for download|autor=Tobias Preis|fecha=24 Mayo, 2012}}</ref> Compararon el índice de orientación futura con el PIB per cápita de cada país y encontraron una fuerte tendencia en los países donde los usuarios de Google informan más sobre el futuro para tener un PIB más alto. Los resultados sugieren que potencialmente puede haber una relación entre el éxito económico de un país y el comportamiento de búsqueda de información de sus ciudadanos capturado en Big Data.

[[Tobias Preis]] y sus colegas [[Helen Susannah Moat]] y [[H. Eugene Stanley]] introdujeron un método para identificar los precursores en línea de los movimientos bursátiles, utilizando estrategias de negociación basadas en los datos del volumen de búsquedas provistos por [[tendencias de búsqueda de Google|Google Trends]].<ref name="Sin_nombre-pF7L-1">{{Cita publicación|url=https://www.nature.com/news/counting-google-searches-predicts-market-movements-1.12879|título=Counting Google searches predicts market movements|apellidos=Ball|nombre=Philip|fecha=26 de abril de 2013|publicación=Nature|fechaacceso=29 de septiembre de 2018|idioma=en|issn=1476-4687|doi=10.1038/nature.2013.12879}}</ref> Su análisis del volumen de búsqueda de [[Google]] para 98 términos de relevancia financiera variable, publicado en [[Scientific Reports]],<ref>{{Cita publicación|url=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3635219/|título=Quantifying Trading Behavior in Financial Markets Using Google Trends|apellidos=Preis|nombre=Tobias|apellidos2=Moat|nombre2=Helen Susannah|fecha=25 de abril de 2013|publicación=Scientific Reports|volumen=3|fechaacceso=29 de septiembre de 2018|issn=2045-2322|doi=10.1038/srep01684|pmc=3635219|pmid=23619126|apellidos3=Stanley|nombre3=H. Eugene}}</ref> sugiere que los aumentos en el volumen de búsqueda para términos de búsqueda relevantes financieramente tienden a preceder grandes pérdidas en los mercados financieros.<ref>{{Cita noticia|apellidos=Bilton|nombre=Nick|título=Google Search Terms Can Predict Stock Market, Study Finds|url=https://bits.blogs.nytimes.com/2013/04/26/google-search-terms-can-predict-stock-market-study-finds/|fechaacceso=29 de septiembre de 2018|periódico=Bits Blog|idioma=en}}</ref><ref>{{Cita noticia|apellidos=Matthews|nombre=Christopher|título=Trouble With Your Investment Portfolio? Google It!|url=http://business.time.com/2013/04/26/trouble-with-your-investment-portfolio-google-it/|fechaacceso=29 de septiembre de 2018|periódico=Time|issn=0040-781X|idioma=en-US}}</ref><ref name="Sin_nombre-pF7L-1"/><ref>{{Cita web|url=https://www.bloomberg.com/news/articles/2013-04-25/big-data-researchers-turn-to-google-to-beat-the-markets|título="'Big Data' Researchers Turn to Google to Beat the Markets"|fechaacceso=29 de septiembre de 2018|autor=Bernhard Warner|fecha=25 de marzo de 2013|sitioweb=www.bloomberg.com}}</ref><ref>{{Cita noticia|título=Hamish McRae: Need a valuable handle on investor sentiment? Google it|url=https://www.independent.co.uk/news/business/comment/hamish-mcrae/hamish-mcrae-need-a-valuable-handle-on-investor-sentiment-google-it-8590991.html|fechaacceso=29 de septiembre de 2018|periódico=The Independent|idioma=en-GB}}</ref><ref>{{Cita web|url=https://www.ft.com/content/e5d959b8-acf2-11e2-b27f-00144feabdc0|título=Subscribe to read|fechaacceso=29 de septiembre de 2018|autor=Richard Waters|fecha=25 de abril de 2013|sitioweb=Financial Times|idioma=en-GB}}</ref>

Los grandes conjuntos de datos vienen con desafíos [[algoritmo|algorítmicos]] que anteriormente no existían. Por lo tanto, existe una necesidad de cambiar fundamentalmente las formas de procesamiento.

Los talleres sobre algoritmos para conjuntos de datos masivos modernos (MMDS) reúnen a científicos informáticos, estadísticos, matemáticos y profesionales del análisis de datos para analizar los desafíos algorítmicos del big data.<ref>{{Cita web|url=http://web.stanford.edu/group/mmds/|título=Workshop on Algorithms for Modern Massive Data Sets (MMDS)|fechaacceso=29 de septiembre de 2018|sitioweb=web.stanford.edu}}</ref>

=== Muestreo de datos masivos ===
Una pregunta de investigación importante que se puede hacer sobre los conjuntos de datos grandes es si necesita ver los datos completos para sacar ciertas conclusiones sobre las propiedades de los datos o si una muestra es lo suficientemente buena.

El nombre big data contiene un término relacionado con el tamaño, y esta es una característica importante de big data. Pero el [[muestreo (estadística)|muestreo (estadísticas)]] permite la selección de puntos de datos correctos dentro del conjunto de datos más grande para estimar las características de toda la población. Por ejemplo, hay alrededor de 600 millones de tuits producidos todos los días. ¿Es necesario mirarlos a todos para determinar los temas que se discuten durante el día? ¿Es necesario mirar todos los tuits para determinar el sentimiento sobre cada uno de los temas? En la fabricación de diferentes tipos de datos sensoriales, como acústica, vibración, presión, corriente, voltaje y datos del controlador están disponibles en intervalos de tiempo cortos. Para predecir el tiempo de inactividad, puede que no sea necesario examinar todos los datos, pero una muestra puede ser suficiente. Big data se puede desglosar por varias categorías de puntos de datos, como datos demográficos, [[Psicografía (mercadotecnia)|psicográficos]], de comportamiento y transaccionales. Con grandes conjuntos de puntos de datos, los especialistas en marketing pueden crear y utilizar segmentos de consumidores más personalizados para una orientación más estratégica.

Se han realizado algunos trabajos en algoritmos de muestreo para big data. Se ha desarrollado una formulación teórica para el muestreo de datos de Twitter.<ref>Deepan Palguna; Vikas Joshi; Venkatesan Chakaravarthy; Ravi Kothari & L. V. Subramaniam (2015). ''Analysis of Sampling Algorithms for Twitter''. ''International Joint Conference on Artificial Intelligence''.</ref>

=== Salud y medicina ===
Hacia mediados del 2009, el mundo experimentó una pandemia de [[gripe A]], llamada gripe porcina o H1N1. La web [[Google Flu Trends]]<ref>[https://www.google.org/flutrends/es]</ref> intentó predecirla a partir de los resultados de las búsquedas. Google Flu Trends usaba los datos de las búsquedas de los usuarios que contenían ''síntomas parecidos a la enfermedad de la gripe'' y los agrupó según ubicación y fecha, y pretendía predecir la actividad de la gripe hasta con dos semanas de antelación más que los sistemas tradicionales. Sin embargo, en el 2013 se descubrió que predijo el doble de visitas al médico de las que hubo en realidad. Sus creadores cometieron dos errores: a) la nueva herramienta había generado muchísimo interés en el público, que la consultaba más por curiosidad que por necesidad, lo que generó ruido en la información, y b) los algoritmos de predicción de los buscadores. En un artículo en la revista ''[[Science (revista)|Science]]'' en el 2014, se analizaron los errores cometidos por Google Flu Trends: "querer sustituir con técnicas de datos masivos los métodos más tradicionales y probados de recolección y análisis de datos, en vez de sólo aplicar dichas técnicas como complemento, como hizo Brittany Wenger con Cloud4cancer". Google Flu Trends dejó de funcionar.<ref name="Sin_nombre-pF7L-2">{{cita publicación|apellidos1=Rodríguez Manzano|nombre1=Anayansi|título=El uso de los datos masivos para salvar vidas|publicación=¿Cómo ves?|fecha=diciembre del 2018|volumen=21|número=241|páginas=16-19|editorial=Dirección General de Divulgación de la Ciencia (UNAM)|ubicación=Ciudad de México|idioma=español}}</ref>

Más concretamente, en Nueva Zelanda<ref>{{cita publicación|apellidos1=Wilson|nombre1=N|apellidos2=Mason|nombre2=M|apellidos3=Tobias|nombre3=M|apellidos4=Peacey|nombre4=M|apellidos5=Huang|nombre5=Q S|apellidos6=Baker|nombre6=M|título=Interpreting “Google Flu Trends” data for pandemic H1N1 influenza: The New Zealand Experience|publicación=Eurosurveillance Edition 2009|fecha=Eurosurveillance Edition 2009|volumen=14 / Issue 44|número=4|url=http://www.eurosurveillance.org/ViewArticle.aspx?ArticleId=19386}}</ref> cruzaron los datos de las tendencias de gripe de Google con datos existentes de los sistemas de salud nacionales, y comprobaron que estaban alineados. Los gráficos mostraron una [[correlación]] con las búsquedas de síntomas relacionados con la gripe y la extensión de la [[pandemia]] en el país. Los países con sistemas de predicción poco desarrollados pueden beneficiarse de una predicción fiable y pública para abastecer a su población de las medidas de seguridad oportunas.

Entre 1853 y 1854, en Londres, una epidemia de cólera mató a miles de personas. El médico [[John Snow]] estudió los registros de defunciones, y descubrió que la mayor parte de los casos se presentaron en un barrio específico: las personas habían bebido agua de un mismo pozo. Cuando lo clausuraron, el número de casos comenzó a disminuir.<ref name="Sin_nombre-pF7L-2"/>

En el 2012, en la [[Feria de Ciencias de Google]], [[Brittany Wenger]], estudiante de 18 años, presentó el proyecto de diseño de un software para ayudar al diagnóstico temprano del [[cáncer de mama]]. Denominó a la plataforma ''[[Cloud4cancer]]'', que utiliza una red de [[inteligencia artificial]] y las [[bases de datos]] de los hospitales para diferenciar una muestra de un tejido benigno de una de un tumor maligno. El sistema inteligente diseñado por Wenger distingue en segundos los dos tipo de tumores, ingresando a la plataforma las características observadas. Es posible que este sistema se aplique más adelante a otros padecimientos, como la [[leucemia]].<ref name="Sin_nombre-pF7L-2"/>

=== Defensa y seguridad ===
Para incrementar la seguridad frente a los ataques de las propias organizaciones, ya sean empresas en el entorno económico o los propios ministerios de defensa en el entorno de [[ciberataques]], se contempla la utilidad de las tecnologías de ''big data'' en escenarios como la vigilancia y seguridad de fronteras, lucha contra el terrorismo y crimen organizado, contra el fraude, planes de seguridad ciudadana o planeamiento táctico de misiones e [[inteligencia militar]].<ref>{{cita publicación|apellidos1=Carrillo Ruiz|nombre1=Jose Antonio|apellidos2=Marco de Lucas|nombre2=Jesus E.|apellidos3=Cases Vega|nombre3=Fernando|apellidos4=Dueñas Lopez|nombre4=Juan Carlos|apellidos5=Cristino Fernandez|nombre5=Jose|apellidos6=Gonzalez Muñoz de Morales|nombre6=Guillermo|apellidos7=Pereda Laredo|nombre7=Luis Fernando|título=Big Data en los entornos de Defensa y Seguridad|publicación=Instituto Español de Estudios Estratégicos|fecha=Marzo de 2013|url=http://www.ieee.es/Galerias/fichero/docs_investig/DIEEEINV03-2013_Big_Data_Entornos_DefensaSeguridad_CarrilloRuiz.pdf}}</ref>

=== Caso específico del proyecto Aloja ===
[[Archivo:Bsc-nvidia-gpu.jpg|250px|miniaturadeimagen|derecha|Una de las máquinas del Marenostrum, [[Supercomputador]] del [[Barcelona Supercomputing Center|BSC]]]]
El proyecto Aloja<ref>{{cita noticia|título=El BSC y Microsoft Research Center optimizarán Big Data con el proyecto Aloja|url=http://www.computing.es/infraestructuras/noticias/1075853001801/bsc-microsoft-research-center-optimizaran.1.html|fechaacceso=1 de enero de 2015|agencia=Computing|fecha=31 de julio de 2014}}</ref> ha sido iniciado por una apuesta en común del [[Barcelona Supercomputing Center]] (BSC) y [[Microsoft]] Research. El objetivo de este proyecto de ''big data'' quiere «conseguir una optimización automática en despliegues de [[Hadoop]] en diferentes infraestructuras».

=== Caso específico de sostenibilidad ===

Conservation International es una organización con el propósito de concienciar a la sociedad de cuidar el entorno de una manera responsable y sostenible. Con la ayuda de la plataforma Vertica Analytics de [[HP]], han situado 1000 cámaras a lo largo de dieciséis bosques en cuatro continentes. Estas cámaras incorporan unos [[sensor]]es, y a modo de cámara oculta graban el comportamiento de la fauna. Con estas imágenes y los datos de los [[sensor]]es (precipitaciones, temperatura, humedad, solar…) consiguen información sobre cómo el [[cambio climático]] o el desgaste de la tierra afecta en su comportamiento y desarrollo.<ref>{{cita publicación|nombre1=CIO|título=How Big Data Is Helping to Save the Planet.|fecha=15 de septiembre de 2014|url=http://www.cio.com/article/2683133/big-data/how-big-data-is-helping-to-save-the-planet.html?source=CIONLE_nlt_insider_2014-09-16#tk.rss_dataanalytics}}</ref>

== Críticas ==
Las críticas al paradigma del big data vienen en dos formas, aquellas que cuestionan las implicaciones del enfoque en sí mismo, y las que cuestionan la forma en que se realiza actualmente.<ref>Kimble, C.; Milolidakis, G. (2015). "Big Data and Business Intelligence: Debunking the Myths". ''Global Business and Organizational Excellence''. '''35''' (1): 23–34. arXiv:1511.03085. doi:10.1002/joe.21642.</ref> Un enfoque de esta crítica es el campo de los estudios de datos críticos.

=== Críticas al paradigma de los grandes datos ===
"Un problema crucial es que no sabemos mucho sobre los microprocesos empíricos subyacentes que conducen a la aparición de las [se] características de red típicas de Big Data".<ref>Snijders, C.; Matzat, U.; Reips, U.-D. (2012). "'[http://www.ijis.net/ijis7_1/ijis7_1_editorial.pdf Big Data': Big gaps of knowledge in the field of Internet]". ''International Journal of Internet Science''. '''7''': 1–5.</ref> En su crítica, Snijders, Matzat y Reips señalan que a menudo se hacen suposiciones muy fuertes sobre las propiedades matemáticas que pueden no reflejar en absoluto lo que realmente está sucediendo a nivel de los microprocesos. Mark Graham ha criticado ampliamente la afirmación de [[Chris Anderson]] de que los macrodatos marcarán el final de la teoría:<ref>{{Cita noticia|apellidos=Anderson|nombre=Chris|título=The End of Theory: The Data Deluge Makes the Scientific Method Obsolete|url=https://www.wired.com/2008/06/pb-theory/|fecha=23 de junio de 2008|fechaacceso=29 de septiembre de 2018|periódico=WIRED|idioma=en-US}}</ref> centrándose en particular en la noción de que los macrodatos siempre deben contextualizarse en sus contextos sociales, económicos y políticos.<ref>{{Cita web|url=http://www.theguardian.com/news/datablog/2012/mar/09/big-data-theory|título=Big data and the end of theory?|fechaacceso=29 de septiembre de 2018|apellido=Graham|nombre=Mark|fecha=9 de marzo de 2012|sitioweb=the Guardian|idioma=en}}</ref> Incluso cuando las empresas invierten sumas de ocho y nueve cifras para obtener información de la transmisión de información de proveedores y clientes, menos del 40{{esd}}% de los empleados tienen procesos y habilidades suficientemente maduros para hacerlo. Para superar este déficit de perspicacia, los grandes datos, sin importar cuán exhaustivos o bien analizados, se complementen con un "gran juicio", según un artículo de Harvard Business Review.<ref>{{Cita noticia|título=Good Data Won’t Guarantee Good Decisions|url=https://hbr.org/2012/04/good-data-wont-guarantee-good-decisions|fecha=1 de abril de 2012|fechaacceso=29 de septiembre de 2018|periódico=Harvard Business Review}}</ref>

En la misma línea, se ha señalado que las decisiones basadas en el análisis de big data inevitablemente "están informadas por el mundo como lo fueron en el pasado o, en el mejor de los casos, como lo es actualmente". Alimentados por una gran cantidad de datos sobre experiencias pasadas, los algoritmos pueden predecir el desarrollo futuro si el futuro es similar al pasado.<ref>{{Obra citada|título=Big data requires big visions for big change {{!}} Martin Hilbert {{!}} TEDxUCL|apellidos=TEDx Talks|url=https://www.youtube.com/watch?v=UXef6yfJZAI|fechaacceso=29 de septiembre de 2018|fecha=13 de enero de 2015}}</ref>  Si la dinámica de sistemas del futuro cambia (si no es un [[proceso estacionario]]), el pasado puede decir poco sobre el futuro. Para hacer predicciones en entornos cambiantes, sería necesario tener un conocimiento profundo de la dinámica de los sistemas, que requiere teoría. Como respuesta a esta crítica, Alemany Oliver y Vayre sugirieron usar el "razonamiento abductivo como un primer paso en el proceso de investigación para traer contexto a las huellas digitales de los consumidores y hacer que emerjan nuevas teorías".<ref>{{Cita publicación|url=https://link.springer.com/article/10.1057/jma.2015.1|título=Big data and the future of knowledge production in marketing research: Ethics, digital traces, and abductive reasoning|apellidos=Alemany Oliver|nombre=Mathieu|apellidos2=Vayre|nombre2=Jean- Sébastien|fecha=2015-03|publicación=Journal of Marketing Analytics|volumen=3|número=1|páginas=5–13|fechaacceso=29 de septiembre de 2018|idioma=en|issn=2050-3318|doi=10.1057/jma.2015.1}}</ref>  Además, se ha sugerido combinar enfoques de big data con simulaciones por computadora, tales como modelos basados en agentes y [[Sistema complejo|Sistemas Complejos.]] Los modelos basados en agentes son cada vez mejores para predecir el resultado de las complejidades sociales de escenarios futuros incluso desconocidos a través de simulaciones por computadora que se basan en una colección de algoritmos mutuamente interdependientes.<ref>{{Cita noticia|apellidos=Rauch|nombre=Jonathan|título=Seeing Around Corners|url=https://www.theatlantic.com/magazine/archive/2002/04/seeing-around-corners/302471/|fecha=1 de abril de 2002|fechaacceso=29 de septiembre de 2018|periódico=The Atlantic|idioma=en-US}}</ref><ref>Epstein, J. M., & Axtell, R. L. (1996). Growing Artificial Societies: Social Science from the Bottom Up. A Bradford Book.</ref> Finalmente, el uso de métodos multivariantes que exploran la estructura latente de los datos, como el [[análisis factorial]] y el [[análisis de conglomerados]], han demostrado ser útiles como enfoques analíticos que van más allá de los enfoques bi-variados (tablas cruzadas) típicamente empleados con conjuntos de datos más pequeños.

En salud y biología, los enfoques científicos convencionales se basan en la experimentación. Para estos enfoques, el factor limitante es la información relevante que puede confirmar o refutar la hipótesis inicial.<ref>{{Cita web|url=https://www.bigdataparis.com/documents/Pierre-Delort-INSERM.pd|título=Accueil - Big Data Paris 2019|fechaacceso=29 de septiembre de 2018|sitioweb=Big Data Paris 2019|idioma=francés}}</ref> Ahora se acepta un nuevo postulado en ciencias biológicas: la información provista por los datos en grandes volúmenes (ómicas) sin hipótesis previas es complementaria y a veces necesaria para los enfoques convencionales basados en la experimentación.<ref>{{Cita publicación|url=https://www.researchgate.net/publication/283298499_BIG_DATA_IN_BIOSCIENCES|título=BIG DATA IN BIOSCIENCES|apellidos=Tambe|nombre=Sanjeev|fecha=29 de octubre de 2015|fechaacceso=29 de septiembre de 2018|doi=10.13140/RG.2.1.3685.0645}}</ref> En los enfoques masivos, la formulación de una hipótesis relevante para explicar los datos es el factor limitante.<ref name="Sin_nombre-pF7L-3">{{Cita web|url=https://www.ft.com/content/21a6e7d8-b479-11e3-a09a-00144feabdc0|título=Big data: are we making a big mistake?|fechaacceso=29 de septiembre de 2018|apellido=Harford|nombre=Tim|fecha=28 de marzo de 2014|sitioweb=Financial Times|idioma=en-GB}}</ref> La lógica de búsqueda se invierte y se deben considerar los límites de la inducción ("Gloria de la ciencia y el escándalo de la filosofía", C. D. Broad, 1926).

Los defensores de la [[Privacidad en Internet|privacidad]] están preocupados por la amenaza a la privacidad que representa el aumento del almacenamiento y la integración de la información de identificación personal; los paneles de expertos han publicado varias recomendaciones de políticas para adaptar la práctica a las expectativas de privacidad.<ref>{{Cita noticia|título=Don’t Build a Database of Ruin|url=http://blogs.hbr.org/cs/2012/08/dont_build_a_database_of_ruin.html|fecha=23 de agosto de 2012|fechaacceso=29 de septiembre de 2018|periódico=Harvard Business Review}}</ref><ref>{{Cita noticia|apellidos=Bond-Graham|nombre=Darwin|título=Iron Cagebook|url=http://www.counterpunch.org/2013/12/03/iron-cagebook/|fecha=3 de diciembre de 2013|fechaacceso=29 de septiembre de 2018|periódico=www.counterpunch.org|idioma=en-us}}</ref><ref>{{Cita noticia|apellidos=Bond-Graham|nombre=Darwin|título=Inside the Tech industry’s Startup Conference|url=http://www.counterpunch.org/2013/09/11/inside-the-tech-industrys-startup-conference/|fecha=11 de septiembre de 2013|fechaacceso=29 de septiembre de 2018|periódico=www.counterpunch.org|idioma=en-us}}</ref> El uso indebido de Big Data en varios casos por los medios, las empresas e incluso el gobierno ha permitido la abolición de la confianza en casi todas las instituciones fundamentales que sostienen a la sociedad.<ref>{{Cita web|url=https://www.theperspective.com/debates/the-perspective-on-big-data/|título=Is Big Data being used for good?|fechaacceso=29 de septiembre de 2018|apellido=Goldring|nombre=Kira|sitioweb=theperspective.com/}}</ref>

Nayef Al-Rodhan sostiene que se necesitará un nuevo tipo de contrato social para proteger las libertades individuales en un contexto de Big Data y corporaciones gigantes que poseen grandes cantidades de información. El uso de Big Data debería supervisarse y regularse mejor a nivel nacional e internacional.<ref>{{Cita web|url=http://hir.harvard.edu/the-social-contract-2-0-big-data-and-the-need-to-guarantee-privacy-and-civil-liberties/|título=The Social Contract 2.0: Big Data and the Need to Guarantee Privacy and Civil Liberties {{!}} Harvard International Review|fechaacceso=29 de septiembre de 2018|autor=Al-Rodhan, Nayef|fecha=16 de septiembre de 2014|sitioweb=hir.harvard.edu|idioma=en-us|urlarchivo=https://web.archive.org/web/20170413090835/http://hir.harvard.edu/the-social-contract-2-0-big-data-and-the-need-to-guarantee-privacy-and-civil-liberties/|fechaarchivo=13 de abril de 2017}}</ref> Barocas y Nissenbaum argumentan que una forma de proteger a los usuarios individuales es informando sobre los tipos de información que se recopila, con quién se comparte, bajo qué limitaciones y con qué fines.<ref>{{Cita libro|apellidos=Barocas|nombre=Solon|título=Privacy, Big Data, and the Public Good|url=https://dx.doi.org/10.1017/CBO9781107590205.004|fechaacceso=29 de septiembre de 2018|fecha=2014/06|editorial=Cambridge University Press|isbn=9781107590205|doi=10.1017/cbo9781107590205.004|páginas=44–75|idioma=en|apellidos2=Nissenbaum|nombre2=Helen|apellidos3=Lane|nombre3=Julia|apellidos4=Stodden|nombre4=Victoria|apellidos5=Bender|nombre5=Stefan|apellidos6=Nissenbaum|nombre6=Helen}}</ref>

=== Críticas del modelo 'V' ===
El modelo 'V' de Big Data es concertante ya que se centra en la escalabilidad computacional y carece de una pérdida en torno a la perceptibilidad y la comprensibilidad de la información. Esto llevó al marco de [[Cognitive Big Data]], que caracteriza la aplicación Big Data de acuerdo con:<ref>{{Cita publicación|url=https://www.researchgate.net/publication/304784955_A_COMPREHENSIVE_SURVEY_ON_BIG-DATA_RESEARCH_AND_ITS_IMPLICATIONS_-_WHAT_IS_REALLY_'NEW'_IN_BIG_DATA_-IT'S_COGNITIVE_BIG_DATA|título=A COMPREHENSIVE SURVEY ON BIG-DATA RESEARCH AND ITS IMPLICATIONS – WHAT IS REALLY 'NEW' IN BIG DATA? -IT'S COGNITIVE BIG DATA!|apellidos=Lugmayr|nombre=Artur|apellidos2=Stockleben|nombre2=Bjoern|fecha=1 de junio de 2016|fechaacceso=29 de septiembre de 2018|apellidos3=Scheib|nombre3=Christoph|apellidos4=Mailaparampil|nombre4=Mathew|apellidos5=Mesia|nombre5=Noora|apellidos6=Ranta|nombre6=Hannu}}</ref>

* Completar los datos: comprensión de lo no obvio de los datos;
* Correlación de datos, causalidad y predictibilidad: la causalidad como requisito no esencial para lograr la previsibilidad;
* Explicación e interpretación: los seres humanos desean comprender y aceptar lo que entienden, donde los algoritmos no lo resuelven;
* Nivel de toma de decisiones automatizada: algoritmos que respaldan la toma de decisiones automatizada y el autoaprendizaje algorítmico;

=== Crítica de la novedad ===
Grandes conjuntos de datos han sido analizados por máquinas de computación durante más de un siglo, incluida la analítica del censo estadounidense realizada en 1890 por las máquinas de tarjetas perforadas de [[IBM]] que computaron estadísticas que incluían medias y variaciones de poblaciones en todo el continente. En décadas más recientes, experimentos científicos como el [[Organización Europea para la Investigación Nuclear|CERN]] han producido datos en escalas similares a los "grandes datos" comerciales actuales. Sin embargo, los experimentos científicos han tendido a analizar sus datos utilizando clusters y grids especializados de [[computación de alto rendimiento]] (supercomputación), en lugar de nubes de computadoras básicas baratas como en la ola comercial actual, lo que implica una diferencia en la cultura y la tecnología.

=== Críticas de la ejecución de macrodatos ===
[[Ulf-Dietrich Reips]] y [[Uwe Matzat]] escribieron en 2014 que el big data se había convertido en una "moda" en la investigación científica. La investigadora [[danah boyd]] ha expresado su preocupación sobre el uso de big data en la ciencia, descuidando principios como elegir una muestra representativa por estar demasiado preocupado por manejar grandes cantidades de datos.<ref>{{Cita web|url=http://www.danah.org/papers/talks/2010/WWW2010.html|título="Privacy and Publicity in the Context of Big Data"|fechaacceso=29 de septiembre de 2018|sitioweb=www.danah.org}}</ref> Este enfoque puede generar sesgos en los resultados de una forma u otra. La integración a través de recursos de datos heterogéneos -algunos que pueden considerarse grandes datos y otros no- presenta desafíos logísticos y analíticos formidables, pero muchos investigadores sostienen que tales integraciones probablemente representen las nuevas fronteras más prometedoras en la ciencia.<ref>Jones, MB; Schildhauer, MP; Reichman, OJ; Bowers, S (2006). [http://www.pnamp.org/sites/default/files/Jones2006_AREES.pdf "The New Bioinformatics: Integrating Ecological Data from the Gene to the Biosphere"]. ''Annual Review of Ecology, Evolution, and Systematics''. '''37''' (1)</ref> En el provocativo artículo "Preguntas críticas para Big Data",<ref>Boyd, D.; Crawford, K. (2012). "Critical Questions for Big Data". ''Information, Communication & Society''. '''15''' (5): 662–679.</ref> los autores titulan big data como parte de la mitología: "los grandes conjuntos de datos ofrecen una forma superior de inteligencia y conocimiento [...], con el aura de la verdad, la objetividad y precisión". Los usuarios de big data a menudo "se pierden en el gran volumen de números", y "trabajar con Big Data sigue siendo subjetivo, y lo que cuantifica no necesariamente tiene un reclamo más cercano sobre la verdad objetiva". Los desarrollos recientes en el dominio de BI, como los informes proactivos, apuntan especialmente a mejoras en la usabilidad de big data, a través del [[Filtro (programa)|filtrado]] automatizado de datos y correlaciones no útiles.<ref>[http://www.fortewares.com/Administrator/userfiles/Banner/forte-wares--pro-active-reporting_EN.pdf Failure to Launch: From Big Data to Big Decisions] {{Wayback|url=http://www.fortewares.com/Administrator/userfiles/Banner/forte-wares--pro-active-reporting_EN.pdf |date=20161206145026 }}, Forte Wares.</ref>

El análisis de macrodatos suele ser poco profundo en comparación con el análisis de conjuntos de datos más pequeños. [194] En muchos proyectos de big data, no hay grandes análisis de datos, pero el desafío es [[Extract, transform and load|extraer, transformar y cargar]] parte del preprocesamiento de datos.<ref>{{Cita web|url=http://www.kdnuggets.com/2014/08/interview-michael-berthold-knime-research-big-data-privacy-part2.html|título=Interview: Michael Berthold, KNIME Founder, on Research, Creativity, Big Data, and Privacy, Part 2|fechaacceso=29 de septiembre de 2018|sitioweb=www.kdnuggets.com|idioma=en-US}}</ref>

Big data es una palabra de moda y un "término vago",<ref>{{Cita noticia|título="Big Data" is an overused buzzword and this Twitter bot proves it - SiliconANGLE|url=http://siliconangle.com/blog/2015/10/26/big-data-is-an-over-used-buzzword-and-this-twitter-bot-proves-it/|fecha=26 de octubre de 2015|fechaacceso=29 de septiembre de 2018|periódico=SiliconANGLE|idioma=en-US}}</ref><ref name="Sin_nombre-pF7L-3"/> pero al mismo tiempo una "obsesión" con empresarios, consultores, científicos y medios de comunicación. Las muestras de datos grandes como Google Flu Trends no generaron buenas predicciones en los últimos años, lo que exageró los brotes de gripe en un factor de dos. Del mismo modo, los premios de la Academia y las predicciones electorales basadas únicamente en Twitter fueron más a menudo fuera del objetivo. Los grandes datos a menudo presentan los mismos desafíos que los datos pequeños; agregar más datos no resuelve los problemas de sesgo, pero puede enfatizar otros problemas. En particular, las fuentes de datos como Twitter no son representativas de la población en general, y los resultados extraídos de dichas fuentes pueden dar lugar a conclusiones erróneas. [[Traductor de Google|Google Translate]], que se basa en el análisis estadístico de big data de textos, hace un buen trabajo al traducir páginas web. Sin embargo, los resultados de dominios especializados pueden ser dramáticamente sesgados. Por otro lado, los macrodatos también pueden introducir nuevos problemas, como el [[problema de las comparaciones múltiples]]: la prueba simultánea de un gran conjunto de hipótesis probablemente produzca muchos resultados falsos que erróneamente parecen significativos. Ioannidis argumentó que "la mayoría de los resultados de investigación publicados son falsos"<ref>{{Cita publicación|url=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1182327/|título=Why Most Published Research Findings Are False|apellidos=Ioannidis|nombre=John P. A.|fecha=2005-8|publicación=PLoS Medicine|volumen=2|número=8|fechaacceso=29 de septiembre de 2018|issn=1549-1277|doi=10.1371/journal.pmed.0020124|pmc=1182327|pmid=16060722}}</ref> debido esencialmente al mismo efecto: cuando muchos equipos científicos e investigadores realizan cada uno experimentos (es decir, procesan una gran cantidad de datos científicos, aunque no con big data), la probabilidad de que un resultado "significativo" sea falso crece rápidamente, incluso más cuando se publican resultados positivos. Además, los resultados del análisis de big data son tan buenos como el modelo en el que se basan. En un ejemplo, Big Data participó en el intento de predecir los resultados de las elecciones presidenciales de EE.{{esd}}UU. en 2016<ref>{{Cita noticia|autor=Lohr, Steve; Singer, Natasha|título=How Data Failed Us in Calling an Election|url=https://www.nytimes.com/2016/11/10/technology/the-data-said-clinton-would-win-why-you-shouldnt-have-believed-it.html|fecha=10 de noviembre de 2016|fechaacceso=29 de septiembre de 2018|idioma=en}}</ref> con diversos grados de éxito. Forbes predijo: "Si usted cree en el análisis de Big Data, es hora de comenzar a planificar para la presidencia de Hillary Clinton y todo lo que eso implica".<ref>{{Cita noticia|apellidos=Markman|nombre=Jon|título=Big Data And The 2016 Election|url=https://www.forbes.com/sites/jonmarkman/2016/08/08/big-data-and-the-2016-election/#4802f20846d7|fechaacceso=29 de septiembre de 2018|periódico=Forbes|idioma=en}}</ref>

== Virtualización de ''big data'' ==
La virtualización de ''big data'' es una forma de recopilar información de múltiples fuentes en el mismo lugar. El ensamblaje es virtual: a diferencia de otros métodos, la mayoría de los datos permanecen en su lugar y se toman bajo demanda directamente desde el sistema de origen.<ref>{{Cita web|url=https://www.datawerks.com/data-virtualization/|título=What Is Data Virtualization?|fechaacceso=14 de mayo de 2018|sitioweb=www.datawerks.com|idioma=en-US|urlarchivo=https://web.archive.org/web/20180410201808/https://www.datawerks.com/data-virtualization/|fechaarchivo=10 de abril de 2018}}</ref>

== Véase también ==
{{lista de columnas|2|
* [[Ciencia de datos]]
* [[Ciencias de la computación]]
* [[Comisión Federal para la Protección de Riesgos Sanitarios]] ([[Cofepris]])
* [[Dataísmo]]
* [[Epidemiología]]
* [[Farmacovigilancia]]
* [[Hashtag]]
* [[Internet de las cosas]]
* [[Medios sociales]]
* [[Democracia digital]]
* [[Datos abiertos]]
* [[Digital 9]]
}}

== Referencias ==
{{listaref|2}}

== Enlaces externos ==
* [http://www.businessoftware.net/que-es-big-data/ Big Data ofrecido por las grandes empresas (SAP, Oracle, Microsoft y otros)]
* [https://topbigdata.es/ Actualidad del Big Data]
* [https://web.archive.org/web/20140606230915/http://www.winshuttle.es/big-data-historia-cronologica/ Historia cronológica del ''Big Data'' / Una línea del tiempo visual con los principales hitos de la historia del almacenamiento de la información] 
* [http://www.lomasnuevo.net/cloud/ibm-crea-una-universidad-gratuita-de-big-data/ IBM crea una universidad de Big Data para aprender gratis]
* [https://web.archive.org/web/20180304054738/https://www.datawerks.com/big-data-solution/ Real Time Data Access and Total Data Integration]
* [https://www.elobservador.com.uy/nota/lo-mas-buscado-en-wikipedia-en-2018-201911195655 Lo más buscado en Wikipedia en 2018 / Cuáles fueron las personalidades y los eventos más consultados en la enciclopedia en línea]
* [https://www.elobservador.com.uy/nota/los-ojos-uruguayos-de-wikipedia-201292120440 Los ojos uruguayos de Wikipedia (21 de septiembre de 2012)]

{{Control de autoridades}}
[[Categoría:Sistemas de gestión de bases de datos]]